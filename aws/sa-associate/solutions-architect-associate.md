# AWS SA Associate Accelerator
## Links 
- Stream: https://www.twitch.tv/aws_namer_programs 
- Accelerator home page: https://mpa-saa-accelerator-q12023.splashthat.com/
- Learning materials: https://explore.skillbuilder.aws/learn/lp/1651/Solution%2520Architect%2520Associate%2520Accelerator%2520-%2520Partner%2520Learning%2520Plan (I usually have to click it twice and sign in twice...) 
    - Not every video in here is required for certification 
    - The hands on labs essentially book-end the learning objectives for the week. Each week of the program will also have an email come out with objectives for the week 
- official AWS study channel: https://aws.amazon.com/training/twitch/

## Info
- Twitch sessions are recorded. The instructor-led 4 hour courses are NOT recorded. 
- labs and tests require monthly charge. Probably able to reimburse with Octo if necessary. 
- This program is ~3 months
- Once past 85% on learning plan (in week 6), will qualify in the running for raffle for voucher
- Recommend 8-9 hours/week of studying. 1.6-1.8 hours/day of studying weekdays only. Pacing helps maintain knowledge & keep committed 
- watch for easter eggs in links in emails and weekly wed/fri chats 
- get stickers by participating in chat. 
    - call out when he says 'you guys' in chat 

# AWS Technical Essentials
- Every action made in AWS is an API call that is authenticated and authorized. 
    - Management console, CLI, SDKs
- Authentication: ensures the user is who they say they are
- Authorization: what actions a user can perform
- 'main route table' is the route table created by default 
- NACL: firewall at the subnet-level. They're stateless; inbound and outbound ports required to be listed 
- Instance security group: stateful; can remember connections & allow returning ephemeral responses 
- block storage: think little pieces making up a larger file. Object: each file is a single entity 

# DynamoDB review
## How DynamoDB Works
- designed for online transaction processing (OLTP), with known request patterns 
- online analytical processing (OLAP) loads are better handled by SQL
- data stored in tables
- items placed in tables
- attributes: essential the 'key' of an item. aka partition key
    - optionally, sort key can be defined
    - primary key: made up of paritition (+optional sort key); must be unique
    - supported types: number, string, binary (base64 encoded), boolean, null
    - can use sets of numbers, strings, and binaries; sets do not preserve order
- schema flexibility  
- durability/availability
    - data written at least twice in separate facilities
    - 99.99% availability
- eventual consistency: default behavior, w strong consistency available for each read operation. best practice to design around eventual consistency 
- RCU (read capacity unit): consumed while reading an item up to 4KB in size each second
    - single item can never be read at more than 3000 RCU
    - eventually consistent reads are 1/2 cost of strongly consistent; 2 4KB EC consumes only 1 RCU
- WCU write capacity unit: consumed while writing an item up to 1KB in size each second 
    - updating a single attribute in an item requires writing the entire item 
    - single item can never be read at more than 1000 WCU
- able to burst and use 'adaptive capacity'
### Basic item requests
- PutItem
- UpdateItem
- DeleteItem (costs the same number of WCUs to delete as to create)
- GetItem
- BatchWriteItem/BatchGetItem
- Scan: scans entire table; not to be done often; can consume all provisioned throughput of table if done poorly
- Query: specify partition key & sort key expression to match 
    - sums the size of all items in the result set & rounds to the nearest 4KB, instead of counting sizes for each individual item identified in the search
### Indexes
- secondary index: direct Query and Scan calls to index instead of base table
- LSI (local secondary index): local to a particular partition key; has the same partition key as the base table 
    - use sparingly; each index will result in add'l writes
    - limit collection size for partition key to ~10GB of data
- GSI (global secondary index): think of it as a completely separate table that DDB replicates to from the base table
    - created & deleted at will
    - generally recommended; but don't build indexes that aren't needed
### Streams
- strictly ordered flow of information according to the changes to the table
- durable & kept up to 24hrs

## Operating DynamoDB
- 400 errors addressed by user
- 500 errors problems DDB will take care of 
- auto scales in response to actual traffic patterns 
    - set RCU and WCU man, max, and target utilization %
- global tables: DDB tables operated across multiple regions 
    - 5 9's, and primary reason to use is to provide extremely low latency to global clients 
    - multi-master, conflicts resolved w last-write-wins
    - strong consistency not possible
- TTL (time-to-live): expire old items to keep storage cost low. Doesn't cost WCU
    - w/n a day or two of epoch-formatted defined time, DDB will delete for free 
- DAX (DynamoDB Acclerator): provides an API-compatible cache for DDB tables
    - highly-available cluster of nodes accessible w/n VPC
    - can decrease amount of RCUs required on table and can smooth out spiky/imbalanced read loads 
- backups
    - manual
    - PITR (point in time recovery): keeps 35-day rolling window of information about table 

## Design Considerations
- uniform workloads
    - choose a partition key with an even distribution of item data and traffic across hash space. A good partition key has a high 'cardinality' - meaning lots of unique values
        - e.g. in keeping track of user status for a mobile game, UserId would be a good partition key; CountryCode would not
- hot and cold data
    - can delete tables (doesn't require WCUs)
    - consider cold tier storage which uses S3
- items limited to 400kB
- use optimistic locking with version number 

## Assessment Review
- facts about consistency:
    - DAX passes strongly consistent reads through but doesn't cache them
    - stringly consistent reads can be made via a VPC endpoint
    - LSI and GSI both support eventually consistent reads
    - you can make two EC reads (each up to 4KB) for one RCU
    - all successful writes are redundantly stored and durable - there is no eventual or strong consistencyu choice to be made for writes (only reads)
    - LSI can only be defined at time of base table creation - cannot be deleted w/o deleting base table
    - DDB streams cannot be used to audit read activity for a table 
    - optimistic currency control in DDB provides a form of locking; read, transformat, conditionally write, retry as required is a description of the mechanism

# DMS Notes
- AWS DMS > resource management > replication instances 
- create endpoints in AWS DMS portal 
    - target endpoint: 
        - when loading data into a MySQL DB by using AWS DMS, disable foreign key w `initstmt=SET FOREIGN_KEY_CHECKS=0`
    - can test endpoints before creation
- replication task: AWS DMS > migration > database migration tasks
    - identify replication instance, source/target endpoint, migration type
    - when migrating existing data & replicating ongoing changes: source MySQL database requires binary log to be enabled and set to row 
- completing migration
    - read & write from both DBs for a time
    - change app config to use new db
    - once new db is configrmed working, start deleting DMS stuff, like replication task, endpoints, replication instance 
    - delete source db when ready

# Auto Scaling overview 
- auto scaling can support appliation-wide scaling; not just ec2. 
    - Able to manage EC2 + EC2 spot fleet, ECS, Aurora, DynamoDB from same AWS AS policy. Needs to all be made in the same CloudFormation stack 
    - all scalable resources show in the same console when using this service 

# Application Load Balancer
- targets can be in multiple groups 

# Application Discovery Service
- migration tool to identify, map, and store computing inventory
- integrated w AWS migration hub 
    - can filter through agents/connectors in the hub (after discovery)
    - provides data on hardware specifications and OS/OS version
    - provides system process information w/n VMs/instances as well - collects every running process 
    - able to export data to .csv
- agent discovery 
    - requires agent installation on existing instances/VMs/etc 
    - better data collection 
- agentless discovery
    - download VM to hypervisor and that VM collects info
    - less robust data collection 



# Amazon S3 File Gateway
## When to consider hybrid cloud
- based on specific customer use case, where migrating totally to the cloud is unfeasible, but wish to use cloud features 
- hybrid cloud storage: use data on prem & store in AWS 
## Storage Gateway
- helps store on-prem data in the cloud
- storage: S3, glacier, FSx, AWS backup, EBS
- mgmt + monitoring: use Storage Gateway mgmt console to manage & monitor the gateway & associate resources
    - integrates w IAM, KMS, CloudTrail, CloudWatch, EventBridge 
- uses std storage protocols: NFS, SMB, iSCSI, iSCSI VTL
- can be a VM or hardware appliance 
### Types of Storage Gateways
- S3 File Gateway
    - appears as network file share
    - moves data to object format
    - has a local cache
- FSx File Gateway
    - mostly focused on supporting Windows file shares
    - local cache of frequently used data is stored 
    - can replace on-prem NAS
    - uses SMB
- Tape Gateway
    - create virtual tapes in VTL using Storage Gateway console 
    - Available for immediate access through S3
    - Archived tapes stored in S3 Glacier flexible retrieval/deep archive 
- Volume Gateway
    - provides iSCSI target; can create block storage volumes & mount as iSCSI devices from on-prem or EC2 app servers
    - cached mode: primary data written to S3, frequent data cached locally
    - stored mode: primary data stored local & entire dataset avaiable for low-latency access while asynchronously backed up to AWS
### S3 File Gateway
- provides applications a file interface to seamlessly and durably store files as objects in S3 
- primary use cases
    - backing up data to cloud 
        - can use NFS and/or SMB to mount shares directly on db and app servers
        - windows ACL support to control access to backup files
        - incremental backups 
    - archiving long-term, retention-based data 
        - basically all the same points as backing up ^ 
    - building data lakes
        - use up to 64 TB of cache per dateway & set up auto cache refresh @ 5 min intervals 
- how it works
    - NFS: linux
    - SMB: windows 
    - each file share is paried w single S3 bucket & uses appliance's local cache
    - appliance can have multiple NFS and SMB file shares 
    - files written to share become objects in S3 w/ one-to-one mapping b/w files & objects 
    - cache refresh will find objects in S3 bucket that were added, removed, or replaced since gateway last listed bucket's contents & cached them
        - automated refresh based on timer value b/w 5 minutes & 30 days 
            - time reflects contents of that file share no longer than the set time ago
- pricing model
    - pay for only what is used
    - charged for data transferred, type & amount of storage used, and requests made
        - request: data written: up to max $125/gateway/mo. $0.01/GB to write data, first 100 GB free 
        - transfer out from storage gateway to on-prem gateway has tiers of charges 
    - if storage gateway is an appliance, also charge associated with the physical appliance 
- Planning & Designing Deployment
    - deploy on prem on VM, with hardware device, or create with AMI for EC2 
    - once deployed, must communicate w storage gateway service for mgmt & data movement
        - connectivity via
            - public endpoint
            - VPC endpoint
            - FIPS (federal information processing standard) 140-2 compliant endpoint for GovCloud workloads 
        - provide IP address that's either public or accessible w/n current network
        - generate activation key w console & use instead of ip
    - recommended to allocate at least 20% of exisitng file store size as cache storage 
        - max supported size of local cache for gateway on VM is 64TiB
    - Adding file shares
        - after gateway is created & activated
        - can be mounted to linux or windows
        - each file share is associated w a unique S3 bucket or unique prefix on the same bucket 
            - file metadata (such as ownership) stored as S3 object metadata cannot be mapped across different protocols 
    - additional considerations
        - verify region supports file gateway
        - file gateway stores file data in any region where buckets for file shares are located 
        - s3 storage classes
            - s3 standard
            - s3 intelligent-tiering
            - s3 standard-IA
            - s3 one zone-IA
            - recommended to write directly to standard & use lifecycle policies to transition object to other classes 
- working w S3 file gateway
    - read, writes, updates
        - all data xfer'd b/w gateway and AWS is encrypted using SSL. Data xfers done through HTTPS. Objects encrypted w SSE-S3 and optionally SSE-KMS
        - reads
            - when client reads from gateway, uses NFS or SMB
            - for client read requests, cache is checked, and if not there, then fetched from S3 using 'byte-range gets' to better use bandwidth 
            - storage gateway service retrieves data from S3 & sends to gateway appliance. Appliance receives data, stores in local cache & provides to client 
        - writes
            - when client writes to gateway, stored locally. Data is compressed asynchronously, changed data uploaded 
            - uses multipart uploads to asynchronously update objects 
        - write updates
            - gateway uses multipart uploads and copy put, so only changed data is uploaded to S3, and data in the cloud is used to create new version of the object 
    - managing the gateway
        - adding file share
            - when created, there are no shares
            - max of 10 shares
            - each share needs to be connected to S3 bucket & given access via IAM role w trust policy
        - file share health: available | creating | updating | deleting | force_deleting | unavailable
        - file share actions 
            - after creating file share, can't change bucket, access point, or VPC endpoint settings
            - can edit S3 storage class, edit name, export as read-write or read-only, refresh cache and more 
                - s3 request pricing applies to cache refresh
            - what can be changed is dependent on whether using NFS or SMB
            - storage gateway can invoke EventBridge when file operations are completed 
        - multi-writer best practices (when multiple gateways or fileshares write to same S3 bucket)
            - configure S3 bucket so that only one fie share can write to it 
                - multiple readers is fine, but it's recommended to have multiple clients write to file gateway, and one write gateway communicate w S3 
            - configure separate, unique object prefix for each file share to avoid writing to the same objects simulatenously 
        - upload notification: notifies when all files written to file share are uploaded to S3 
- secure & monitor gateway
    - use IAM identities & policies 
        - storage gateway file share has an IAM role attached, and bucket may have bucket policy as well
            - needs a trust policy (e.g., "Action": "sts:AssumeRole") and permissions policy
            - api actions: ActivateGateway | AddCache | CreateNFSFileShare | ListFileShares | RefreshCache
    - protecting your data 
        - data in the cache is not encrypted
        - data encrypted in transit and in the cloud 
        - NFS
            - on the file share, can limit access to specific clients/networks by IP
            - permit read-only or read-write access
            - activate user permission squashing
        - SMB
            - limit access for AD users only or authenticating guest access 
                - if guest access authentication is configured, POSIX is used for permissions
            - setting file share visibility to read-only or read-write
            - controlling file/dir access by POSIX (portable operating system interface), or fine grained permissions using Windows ACLs
        - access to S3
            - write IAM user policies to govern access
            - write specific bucket policies
            - use S3 block public access to limit public access
            - restrict access to specific actions w S3 object lock & setting guess MIME type and requester pays
    - monitoring and alerting
        - CloudWatch to monitor health of gateway & file shares
        - EventBridge to notify when file ops are done
        - AWS CloudTrail to track user activity

# Amazon S3 Volume Gateway
- provides cloud-backed iSCSI block storage volumes to on-prem apps
- when activated, gateway storage volumes created
    - mapped to on-prem DAS (direct attached storage) or SAN (storage attached network) disks
    - start w new disks or disks already in use
- modes
    - cached volume
        - primary data stored in S3 & retain frequently accessed data locally in cache. Reduces cost for primary (on-prem) storage and gives low-latency access to frequently used data
        - data that is modified is moved to upload buffer, and prepared for asynchronous upload to S3 
        - requires cache storage & upload buffer
    - stored volume
        - store data locally & asynchonously back up point-in-time snapshots to S3 
        - provides durable & affordable offsite backups 
        - only available w on-prem host platform options 
        - only requires an upload buffer (no cache storage)
- to prepare data for upload, gateway stores incoming data in upload buffer, a staging area
    - use DAS or SAN disks for working storage
    - gateway uploads data from upload buffer via SSL to storage gateway in AWS, then stored in S3 as EBS snapshots 
- snapshots of storage volumes can be taken
    - incremental; only capture changes since last snapshot
    - can initiate on schedule or one-time basis
    - to recover, restore EBS snapshot to on-prem gateway storage volume 
- decision factors to use volume gateway
    - want to centrally manage & automate volume snapshots
    - volume sizes steady & predictable
        - volume resizing not supported
            - to decrease storage, new gateway must be created & data migrated to new gateway
            - to increase storage, add new disks to the gateway; can't expand disks previously allocated 
    - are processes that require reading all data on the entire volume used? E.g. virus scans 
        - use stored mode
    - not able to access snapshot data using S3 console or APIs
        - cannot access via S3 console; must use storage gateway or EBS management console 
- pricing
    - pay only for what is used
    - charged based on amount of data xferred out of AWS, type & amount of storage used, requests made
        - if storage gateway deployed via hardware appliance, also cost of appliance 
## Planning & Designing a Volume Gateway Deployment
- Overview
    - Cached volume
        - use S3 as primary data storage
        - frequently accessed data is cached on premesis
        - can range from 1 GiB to 32 TiB
        - each gateway configured for cached volumes can support up to 32 volumes for max 1,024 TiB (1 PiB)
        - use cases
            - custom file shares
                - good for apps w large dataset, but app only needs low-latency access to a subset of that data at a given time
            - migrating app data into S3 & transition to EC2
    - Stored Volume 
        - data stored locally & backed up asyncronously to S3
        - take one-time or scheduled snapshots
        - good for durable off-site backups
            - quick to restore EBS snapshot to EC2
        - range from 1 GiB to 16 TiB
        - each gateway configured for stored volumes supports to up 32 volumes for max volume storage of 512 TiB (.5 PiB)
        - can only do stored volume on prem
        - use cases
            - block storage backups
                - e.g. dataset has large working set that can't be split up & need low latency access all the time
            - migrations or phased migrations
            - cloud-based disaster recovery
    - deploying
        - on prem
            - download VM from AWS 
            - purchase hardware appliance 
        - on AWS
            - deploy storage gateway AMI in EC2
                - supported for cached volume
        - gateway appliance sizing
            - determine number of total volumes & capacity needed
            - estimate app & workload volume
                - minimum requirement
                    - cache storage: minimum of 150 GiB for cached volume
                    - upload buffer storage: minimum of 150 GiB for cached & stored volume
                    - best practice to increase performance is to allocate multiple local disks for cache storage w at least 150 GiB storage
                        - no cache storage allocated for stored volumes 
    - connectivity b/w gateway appliance & service 
        - public endpoint
        - VPC endpoint
        - FIPS 140-2 compliant endpoint for GovCloud 
    - network considerations
        - requires the following ports: 443 | 80 | 53 (DNS) | 22 | 123 (NTP) | 3260 (iSCSI)
    - adding volumes to gateway appliance
        - cache volume
            - use console to provision storage volumes backed by S3 
            - can also use API or SDK
            - mount those storage volumes to on-prem app servers as iSCSI devices
        - stored volume
            - map to on-prem DAS or SAN disks
                - start w new disks or disks already holding data
            - mount those storage volumes to on-prem app servers as iSCSI devices 
    - additional considerations 
        - region
            - verify region supports volume gateway; must be selected before deploying gateway
            - EBS snapshots initially stored in region where volume gateway is created  
                - can back up snapshots in other regions using cross-region copy in AWS backup
        - recommended to configure CHAP (Challenge-Handshake Authentication Protocol) to access iSCSI
## Working with Volume Gateway
- reads, writes, and updates 
    - reads
        - cached volumes
            - if data is in cache volume, served locally & no latency
            - if not incache, compressed data from S3 retreived & sent to gateway appliance
            - appliance decompresses, stored in local cache, & provides to app (known as read-through cache - app always requests data from cache)
        - stored volumes 
            - 100% of data on volumes stored locally. all reads come directly from virtual appliance or SAN on local disk
    - writes
        - cached volumes
            - data written to local volume cache in native format
            - gateway compresses and encrypts data as it moves from cache to upload buffer
            - upload buffer provided so that performance not impacted when reading/writing to cache
            - as write happens, data goes into cache and is write-back; get local ack quickly w low latency & high thruput
            - from upload buffer, data xfer'd to AWS
        - stored volumes
            - happen directly to the disk & immediately acknowledged
            - when snapshot created, data will then be moved to AWS
            - data compressed & encrypted when moved to upload buffer
            - data securely xfer'd to S3
    - working w iSCSI initators 
        - iSCSI (small computer system interface)
            - facilitate data xfer's over intranets & manage storage over long distances
            - applications (called initiators) can send SCSI commands to storage devices (called targets) on remote servers 
        - for volume gateway, iSCSI targets are volumes
            - part of that includes connecting to those targets, customizing iSCSI settings, and configuring CHAP
        - iSCSI initiator
            - client component; sends requests to target
            - can be implmemented in hardware or software; storage gateway only supports software initiators 
        - iSCSI target
            - server component that receives and responds to requests from initators 
            - each volume exposed as target
            - connect one initiator to each target 
- working w volumes
    - once gateway is running, add volumes to associate w gateway
    - volume recovery point
        - point in time in which all data of volume is consistent and can be snapshotted or cloned
    - recovery options
        - volume clone
            - cloning from exisitng volume is faster & more cost effective that creating EBS snapshots
        - EBS snapshots
    - expanding volume size
        - automatic resizing not supported
            - create a snapshot of the volume desired & use snapshot to create new volume of a larger size
            - use cached volume to be expanded to clone a new a new volume of a larger size 
- managing gateway  
    - managing bandwidth for gateway
        - by default, gateway has no rate limits on upload or download
        - can throttle upload throughput from gateway to AWS or download throughput from AWS to gateway
            - gateway console > actions > edit bandwith rate limit 
- optimizing cost & performance 
    - volume storage is billed only for amount of data stored on volume, not size of created volume
        - best practice: delete older volumes & snapshots that are no longer needed
            - deleting a volume does not automatically delete associated snapshots 
            - to reduce storage costs, delete snapshots > 30 days old 
    - performance recommendations
        - optimize iSCSI settings to achieve higher I/O
            - choose 256 KiB for MaxReceiveDataSegmentLength and FirstBurstLength and 1 MiB for MaxBurstLength
        - back gateway virtual disks w separate physical disks
            - e.g., don't provision local disks for upload buffer & cache storage that use same physical disk
        - change volume config
            - if adding more volumes to gateway reduces throughput, consider adding volumes to another gateway
## Secure and Monitor Volume Gateway
- granting permissions
    - resource owner: AWS account of the principal entity that authenticates the request that creates the resource 
    - to grant resource access, gateway assumes a role that's associated w IAM policy that grants the access 
- protecting data
    - data encyprted in transit & in AWS cloud
    - data on gateway cache not encrypted
    - all data transfers b/w gateway appliance & AWS storage encrypted w SSL/TLS
    - all data stored by Storage Gateway in S3 is encrypted with SSE-S3 (default) or SSE-KMS depending on config
    - EBS snapshots encrypted at rest w AES-256
    - CHAP authentication (an iSCSI protocol)
        - authentacte b/w gateway & iSCSI initators 
        - protects against man-in-the-middle and playback attacks by periodically verifying identity of iSCSI initator
        - to set up, configure in Storage Gateway console & iSCSI initiator software used to connec to target 
        - uses mutual CHAP

# AWS Network Connectivity Options
## Connectivity Concepts
- General Concepts 
    - multi-tier architecture
        - often presentation > application > data
    - multi-vpc architecture 
    - High availability: reducing or managing failures and minimzing downtime w redundant/parallel components & eliminate single points of failure (SPOF)
    - Hybrid network
## Understanding AWS Network Service Offerings 
- VPC endpoints 
    - privately connect VPC to supported AWS services & VPC endpoint services
    - resources w/n VPC do not require public IP address to communiate w resources outside VPC
    - traffic b/w VPC & service do no leave Amazon network 
    - does not require gateway, NAT, VPN, or direct connect
    - Types of VPC Endpoints
        - Gateway VPC Endpoints 
            - target specific IP routes in a route table using prefix lists
                - there are specific prefixes for the supported services 
                    - e.g. com.amazonaws.<region>.dynamodb, com.amazonaws.region.s3
                    - list is used for traffic destined to DynamoDB or S3
        - interface endpoints
            - powered by AWS PrivateLink
            - is an ENI w private IP address from subnet CIDR range
            - serves as entry point for traffic destined to supported AWS service or VPC endpoint service
        - gateway load balancer (GLB) endpoint
            - is an ENI w private IP address from subnet CIDR range
            - entry point to intercept rtraffic and route to a service configured using GLB endpoint as target for route on route table
                - e.g., used for security inspection 
            - powered by AWS PrivateLink
    - pricing
        - charged for each hour VPC endpoint is provisioned in each AZ & each GB processed thru VPC endpoint
- AWS PrivateLink
    - private connection b/w VPCs & AWS services
    - provides secure usage w/n AWS network 
        - services establish TCP connection b/w service provider's VPC and service consumer's VPC
    - avoids exposing traffic to public internet 
    - benefits
        - security
            - provides VPCs secure & scalable way to privately connect to AWS hosted services
            - uses private IP addresses and SGs w/n VPC
        - simplification
            - removes need to permit public IPs w IGWs, NAT gateways, or firewalls
            - does not require route table modification
            - not necessary to establish IGW, VPC peering, or transit VPC
        - capabilities
            - gives on-prem network private access to AWS thru Direct Connect 
            - can make services available to other accounts or VPCs that are accessed securely as private endpoints
                - use PrivateLink w NLB to route traffic to app, and clients can connect to any hosted app 
    - considerations
        - doesn't support IPv6
        - communicates w NLB that serves traffic to instances w/n subnet, so all IP addresses logged by app will be private IP addresses from NLB; will not see IP of customer or service consumer 
        - Activate Proxy Protocol v2 on NLB for NLB to send add'l connection info like source & destination (may require changed to app)
        - endpoint services cannot be tagged 
        - endpoint DNS does not resolve outside of VPC
        - endpoint services available in the region which they are created & can be accessed in remote regions using inter-region VPC peering
    - DNS
        - endpoint-specific regional DNS hostname
            - automatically generated & includes unique endpoint identifier, service identifier, region, and `vpce.amazonaws.com` in its name
        - zonal-specific DNS hostname
            - can generate zonal-specific DNS hostname for each AZ that endpoint is available in. Hostname includes AZ in its name
            - support cross-zone load balancing
            - regional data xfer chanrges might apply for data xferred b/w AZs
        - private DNS hostname
            - can use private DNS hostname to alias the auto created regional-specific or zonal-specifc DNS names
- VPC peering
    - networking connection b/w two VPCs that allows private traffic routing
    - benefits
        - HA
            - it's not gateway or reliant on physical hardware; simply helps facilitate xfer of data 
        - inter-region VPC peering: establish peering relationships b/w VPCs across regions 
            - uses IP addresses w/o requiring gateways, VPN connections, or other network appliances
            - traffic encrypted w no SPOF or bandwidth bottleneck & never transverses public internet 
        - can peer b/w different accounts 
    - scenarios 
        - full sharing of resources b/w all VPCs
            - requires *EACH* VPC to be connected to another VPC, one at a time, and all those links have to be maintained
        - partial sharing of centralized resources
            - e.g., VPCs communicate w one security VPC, and each one only has to maintain that one connection 
    - non-valid peering configs
        - overlapping CIDR blocks
        - transitive peering
            - peering from A to B and B to C does not grant A to C
        - edge-to-edge routing thru gateway or private connection
            - if any VPC in peering has the following, can't extend peering to that connection
                - VPN connection or direct connection to corporate network
                - internet connection thru IGW
                - internet connection thru NAT
                - gateway VPC endpoint to AWS service
    - pricing
        - no charge for setting up or running. Data xfer is charged per GB for send & receive, regardless of AZs involved 
- AWS Direct Connect (DC)
    - private, reliable connection from AWS to physical facility
    - fully integrated & redundant service 
    - offers consistent performance w reduced bandwidth cost 
    - speed
        - offers physical connections of 1, 10, 100 Gbps
            - 100 Gbps available only in select locations 
        - supports Link Aggregation Control Protocl (LACP), which allows multiple dedicated physical connections to be grouped into Link Aggregation Groups (LAGs)
            - max 2 100 Gbps connections in a LAG, or 4 connections with port speeds <100 GbPs
            - each connection counts toward overall connection limit for region
            - all connections in the LAG must terminate at same Direct Connect endpoint 
    - network requirements
        - network must be co-located w existing Direct Connect location
            - customer deploys router & supporting networking equipment to location w physical uplink to AWS
            - router at DC location connected to AWS router using cross-connect 
        - working w Direct Connect partner
            - DC partner provides physical equipment to connect to AWS router at Partner's physical location
            - physical link used to config DC service to connect on-prem to AWS
        - working w independent service provider to connect to Direct Connect 
        - technical requirements
            - network must use single-mode fiber
                - 1000BASE-LX transceiver for 1-Gb ethernet
                - 10GBASE-LR transceiver for 10-Gb ethernet
                - 100GBASE-LR4 for 100 Gb
            - port auto-negotiation must be deactivated
                - port speed & full-duplex mode must be configured manually
            - 802.1Q VLAN encapsulation must be supported across entire connection
            - device must support BGP and BGP MD5 authentication
            - (optional) can configure Bidirectional Forwarding Detection (BFD) on network. Asynchronous BFD automatically activated for DC virtual interfaces, but not on-prem until configured on physical device
    - LOA-CFA (Letter of Authorization and Connecting Facility Assignment)
        - must be signed for each new physical connection individually from the DC console 
        - used to show operater of facility hosting AWS router that request approved to use AWS router
    - Virtual interface types
        - private virtual interface
            - permits traffic to be routed to any VPC resource in the same private IP space as the virutal interface
        - public virtual interface
            - permits traffic to be routed to any VPC or AWS regional resouce w a public IP address in same region
        - transit virtual interface
            - permits traffic to be routed to any VPC or AWS regional resouce routable thru AWS Transit Gateway in same regaion
    - pricing
        - pay for what is used; no minimum fee
            - port hour
                - determined by connection type (dedicated or hosted) and capacity
            - outbound data xfer
                - charged per GB
- AWS Site-to-Site VPN
    - enables securely connect on-prem network to Amazon VPC (e.g. branch office site)
    - based on IPsec technology
    - each tunnel terminates in different AZ on AWS side, but must terminate on the same customer gateway on the customer side 
    - customer gateway
        - resource created & configured in AWS that represents on-prem gateway
        - contains info about type of routing, BGP, ASN, and other info
    - customer gateway device
        - physical device or software app on customer side of connection
    - virtual private gateway
        - VPN concentrator on AWS side
    - transit gateway
        - transit hub used to interconnect VPCs and on-prem networks
        - have to use this or VPG
    - limitations
        - IPv6 partially supported; dualstack thru separate tunnels for inner traffic
        - IPv6 for outer tunnel connections not supported
        - does not support Maximum Transmission Unit (MTU discovery)
            - greatest MTU available on inside tunnel interface is 1,399 bytes
        - throughput of all AWS S2S VPN connections limited
            - when terminating on VPG, only one tunnel of pair can be active & carries max of 1.25 Gbps, which is closer to 1 Gbps IRL
            - when terminate on TG, both tunnels in pair can be active & carry max of 2.5 Gbps. IRL, closer to 2 Gbps
            - each flow (e.g., TCP stream) will be limited to max 1.25 Gbps (IRL 1 Gbps)
        - max packets per second (PPS) per tunnel: 140,000
        - AWS S2S VPN terminating on TG supports equal-cost multi-path routing (ECMP) and multi-exit discriminator (MED) across tunnels in same different connections
            - ECMP only supported with TG VPN config
            - MED is used to ID primary tunnel for S2S VPN connections that use BGP
            - BFD not supported on AWS S2S VPN; it IS supported on DC
        - uses public IPv4 addresses & require public virtual interface to transport over DC
            - AWS S2S VPN over private DC not yet available 
        - for globally distributed apps, accelerated S2S VPN option provides connection to global AWS backbone thru AWS Global Accelerator
            - Cannot use global S2S VPN w DC public virtual interface 
        - recommended to avoid overlapping CIDR blocks 
    - pricing
        - AWS S2S VPN
            - connection per hour (varies by region)
            - data xfer out charges (based on EC2 on-demand pricing)
        - Accelerated S2S VPN
            - connection per hour (varies by region)
            - data xfer out charges (based on EC2 on-demand pricing)
            - hourly charges for two AWS Global Accelerators per VPN connection
            - data xfer out premium (DT-Premium) fees
                - depends on source region & edge location (based on Global Accelerator pricing)
- AWS Client VPN
    - enabled secure connection for users to AWS or on-prem networks (e.g. remote employees)
    - managed client-based VPN based on openVPN technology
    - client VPN endpoint
        - VPN admin creates & configs Client VPN endpoint in AWS
        - admin controls networks & resources that are accessible when connection is made 
    - client VPN application
        - software app used to connect to client VPN endpoint & establish connection
    - client VPN endpoint config file
        - provided to user by VPN admin
        - info about endpoint & certs required to establish connection
        - loaded into chosen VPN client app 
    - limitations
        - supports IPv4 only
        - Security Assertion Markup Language (SAML) 2.0-based federated authentication only works w AWS provided client v1.2.0 or later
        - SAML integration w AWS SSO requires workaround. Better implementation being worked on
        - client CIDR ranges must be at least /22 and not greater than /12
        - client VPN endpoint does not support subnet associations in a dedicated tenancy VPC
        - client VPN *NOT* compliant with FIPS
        - client CIDR range cannot overlap w local CIDR of VPC where associated subnet is located
            - cannot overlap any routes manually added to client VPN endpoint route table
        - portion of addresses in client CIDR range is used to support availability of client VPN endpoint; recommended to use CIDR block that contains twice the number of required IP addresses
        - client CIDR range cannot be changed after creating client VPN endpoint
        - subnets associated w client VPN endpoint must be in same VPC
        - cannot associate multiple subnets from same AZ w client VPN endpoint
        - ACM certs not supported w mutual auth since private key can't be extracted
            - can use ACM server as server-side cert
            - generate client-side cert when user has key or use AWS Certificate Manager Private Certificate Authority (ACM PCA) that gives private keys 
                - if customer authenticating based on AD or SAML, can use gneral ACM-generated cert b/c only server cert is required 
    - monitoring
        - cloudwatch
        - can do basic posture assessment w Lambda
    - pricing
        - charged for number of active client connections per hour & number of subnets associated to Client VPN per hour
        - any client connection that is less than an hour is also prorated for the hour
- AWS Transit Gateway
    - HA and scalable service that provides interconnectivity b/w VPCs and on-prem network
    - w/n region, provides option for consolidating & centrally managing routing b/w VPCs w hub-and-spoke network architecture 
    - b/w regions, supports inter-region peering w other transit gateways 
        - facilitates routing network traffic b/w VPCs of different regions over AWS global backbone 
        - traffic not routed over the internet 
    - key concepts
        - supported attachments
            - one or more VPCs
            - compatible Software-Defined Wide Area Network (SD-WAN) appliance
            - DC gateway
            - peering connection w another TG
            - VPN connection to a TG
        - TG MTU
            - 8,500 bytes for
                - VPC connections
                - DC connections
                - connects w other TGs
                - peering connections
            - 1,500 bytes for
                - VPN connections 
        - TG route table
            - has default route table 
            - can optionally have add'l route table
            - includes dynamic & static routes that decide next hop based on destination IP of packet
            - target of routes can be any TG attachment
        - Associations
            - each attachment associated w exactly one RT
            - each RT can be associated w zero or many attachments 
        - route propogation 
            - VPC, VPN connection, or DC gateway can dynamically propagate routes to TG RT
                - DC attachment: routes are propogated to TG RT by default
                - VPC: must create static routes to send traffic to TG
                - VPN or DC gateway: routes propagated from TF to on-prem router using BGP
                - peering attachment: must create static route in TG RT to point to peering attachment
    - inter-regional peering
        - two types of peering connection for VPCs in different regions: VPC peering, TG peering
            - both are one-to-one
            - TG are simpler network design & consolidated mgmt 
    - pricing
        - charges for number of connections per hour and per GB of data processed
            - see AWS TG pricing page 
- peering vs TG:
    - peering is cheaper and less latency
    - TG is less complex 