# AWS SA Associate Accelerator
## Links 
(I usually have to click it twice and sign in twice...) 
    - Not every video in here is required for certification 
    - The hands on labs essentially book-end the learning objectives for the week. Each week of the program will also have an email come out with objectives for the week 
- official AWS study channel: https://aws.amazon.com/training/twitch/

## Info
- Twitch sessions are recorded. The instructor-led 4 hour courses are NOT recorded. 
- labs and tests require monthly charge. Probably able to reimburse with Octo if necessary. 
- This program is ~3 months
- Once past 85% on learning plan (in week 6), will qualify in the running for raffle for voucher
- Recommend 8-9 hours/week of studying. 1.6-1.8 hours/day of studying weekdays only. Pacing helps maintain knowledge & keep committed 
- watch for easter eggs in links in emails and weekly wed/fri chats 
- get stickers by participating in chat. 
    - call out when he says 'you guys' in chat 

# AWS Technical Essentials
- Every action made in AWS is an API call that is authenticated and authorized. 
    - Management console, CLI, SDKs
- Authentication: ensures the user is who they say they are
- Authorization: what actions a user can perform
- 'main route table' is the route table created by default 
- NACL: firewall at the subnet-level. They're stateless; inbound and outbound ports required to be listed 
- Instance security group: stateful; can remember connections & allow returning ephemeral responses 
- block storage: think little pieces making up a larger file. Object: each file is a single entity 

# DynamoDB review
## How DynamoDB Works
- designed for online transaction processing (OLTP), with known request patterns 
- online analytical processing (OLAP) loads are better handled by SQL
- data stored in tables
- items placed in tables
- attributes: essential the 'key' of an item. aka partition key
    - optionally, sort key can be defined
    - primary key: made up of paritition (+optional sort key); must be unique
    - supported types: number, string, binary (base64 encoded), boolean, null
    - can use sets of numbers, strings, and binaries; sets do not preserve order
- schema flexibility  
- durability/availability
    - data written at least twice in separate facilities
    - 99.99% availability
- eventual consistency: default behavior, w strong consistency available for each read operation. best practice to design around eventual consistency 
- RCU (read capacity unit): consumed while reading an item up to 4KB in size each second
    - single item can never be read at more than 3000 RCU
    - eventually consistent reads are 1/2 cost of strongly consistent; 2 4KB EC consumes only 1 RCU
- WCU write capacity unit: consumed while writing an item up to 1KB in size each second 
    - updating a single attribute in an item requires writing the entire item 
    - single item can never be read at more than 1000 WCU
- able to burst and use 'adaptive capacity'
### Basic item requests
- PutItem
- UpdateItem
- DeleteItem (costs the same number of WCUs to delete as to create)
- GetItem
- BatchWriteItem/BatchGetItem
- Scan: scans entire table; not to be done often; can consume all provisioned throughput of table if done poorly
- Query: specify partition key & sort key expression to match 
    - sums the size of all items in the result set & rounds to the nearest 4KB, instead of counting sizes for each individual item identified in the search
### Indexes
- secondary index: direct Query and Scan calls to index instead of base table
- LSI (local secondary index): local to a particular partition key; has the same partition key as the base table 
    - use sparingly; each index will result in add'l writes
    - limit collection size for partition key to ~10GB of data
- GSI (global secondary index): think of it as a completely separate table that DDB replicates to from the base table
    - created & deleted at will
    - generally recommended; but don't build indexes that aren't needed
### Streams
- strictly ordered flow of information according to the changes to the table
- durable & kept up to 24hrs

## Operating DynamoDB
- 400 errors addressed by user
- 500 errors problems DDB will take care of 
- auto scales in response to actual traffic patterns 
    - set RCU and WCU man, max, and target utilization %
- global tables: DDB tables operated across multiple regions 
    - 5 9's, and primary reason to use is to provide extremely low latency to global clients 
    - multi-master, conflicts resolved w last-write-wins
    - strong consistency not possible
- TTL (time-to-live): expire old items to keep storage cost low. Doesn't cost WCU
    - w/n a day or two of epoch-formatted defined time, DDB will delete for free 
- DAX (DynamoDB Acclerator): provides an API-compatible cache for DDB tables
    - highly-available cluster of nodes accessible w/n VPC
    - can decrease amount of RCUs required on table and can smooth out spiky/imbalanced read loads 
- backups
    - manual
    - PITR (point in time recovery): keeps 35-day rolling window of information about table 

## Design Considerations
- uniform workloads
    - choose a partition key with an even distribution of item data and traffic across hash space. A good partition key has a high 'cardinality' - meaning lots of unique values
        - e.g. in keeping track of user status for a mobile game, UserId would be a good partition key; CountryCode would not
- hot and cold data
    - can delete tables (doesn't require WCUs)
    - consider cold tier storage which uses S3
- items limited to 400kB
- use optimistic locking with version number 

## Assessment Review
- facts about consistency:
    - DAX passes strongly consistent reads through but doesn't cache them
    - stringly consistent reads can be made via a VPC endpoint
    - LSI and GSI both support eventually consistent reads
    - you can make two EC reads (each up to 4KB) for one RCU
    - all successful writes are redundantly stored and durable - there is no eventual or strong consistencyu choice to be made for writes (only reads)
    - LSI can only be defined at time of base table creation - cannot be deleted w/o deleting base table
    - DDB streams cannot be used to audit read activity for a table 
    - optimistic currency control in DDB provides a form of locking; read, transformat, conditionally write, retry as required is a description of the mechanism

# DMS Notes
- AWS DMS > resource management > replication instances 
- create endpoints in AWS DMS portal 
    - target endpoint: 
        - when loading data into a MySQL DB by using AWS DMS, disable foreign key w `initstmt=SET FOREIGN_KEY_CHECKS=0`
    - can test endpoints before creation
- replication task: AWS DMS > migration > database migration tasks
    - identify replication instance, source/target endpoint, migration type
    - when migrating existing data & replicating ongoing changes: source MySQL database requires binary log to be enabled and set to row 
- completing migration
    - read & write from both DBs for a time
    - change app config to use new db
    - once new db is configrmed working, start deleting DMS stuff, like replication task, endpoints, replication instance 
    - delete source db when ready

# Auto Scaling overview 
- auto scaling can support appliation-wide scaling; not just ec2. 
    - Able to manage EC2 + EC2 spot fleet, ECS, Aurora, DynamoDB from same AWS AS policy. Needs to all be made in the same CloudFormation stack 
    - all scalable resources show in the same console when using this service 

# Application Load Balancer
- targets can be in multiple groups 

# Application Discovery Service
- migration tool to identify, map, and store computing inventory
- integrated w AWS migration hub 
    - can filter through agents/connectors in the hub (after discovery)
    - provides data on hardware specifications and OS/OS version
    - provides system process information w/n VMs/instances as well - collects every running process 
    - able to export data to .csv
- agent discovery 
    - requires agent installation on existing instances/VMs/etc 
    - better data collection 
- agentless discovery
    - download VM to hypervisor and that VM collects info
    - less robust data collection 



# Amazon S3 File Gateway
## When to consider hybrid cloud
- based on specific customer use case, where migrating totally to the cloud is unfeasible, but wish to use cloud features 
- hybrid cloud storage: use data on prem & store in AWS 
## Storage Gateway
- helps store on-prem data in the cloud
- storage: S3, glacier, FSx, AWS backup, EBS
- mgmt + monitoring: use Storage Gateway mgmt console to manage & monitor the gateway & associate resources
    - integrates w IAM, KMS, CloudTrail, CloudWatch, EventBridge 
- uses std storage protocols: NFS, SMB, iSCSI, iSCSI VTL
- can be a VM or hardware appliance 
### Types of Storage Gateways
- S3 File Gateway
    - appears as network file share
    - moves data to object format
    - has a local cache
- FSx File Gateway
    - mostly focused on supporting Windows file shares
    - local cache of frequently used data is stored 
    - can replace on-prem NAS
    - uses SMB
- Tape Gateway
    - create virtual tapes in VTL using Storage Gateway console 
    - Available for immediate access through S3
    - Archived tapes stored in S3 Glacier flexible retrieval/deep archive 
- Volume Gateway
    - provides iSCSI target; can create block storage volumes & mount as iSCSI devices from on-prem or EC2 app servers
    - cached mode: primary data written to S3, frequent data cached locally
    - stored mode: primary data stored local & entire dataset avaiable for low-latency access while asynchronously backed up to AWS
### S3 File Gateway
- provides applications a file interface to seamlessly and durably store files as objects in S3 
- primary use cases
    - backing up data to cloud 
        - can use NFS and/or SMB to mount shares directly on db and app servers
        - windows ACL support to control access to backup files
        - incremental backups 
    - archiving long-term, retention-based data 
        - basically all the same points as backing up ^ 
    - building data lakes
        - use up to 64 TB of cache per dateway & set up auto cache refresh @ 5 min intervals 
- how it works
    - NFS: linux
    - SMB: windows 
    - each file share is paried w single S3 bucket & uses appliance's local cache
    - appliance can have multiple NFS and SMB file shares 
    - files written to share become objects in S3 w/ one-to-one mapping b/w files & objects 
    - cache refresh will find objects in S3 bucket that were added, removed, or replaced since gateway last listed bucket's contents & cached them
        - automated refresh based on timer value b/w 5 minutes & 30 days 
            - time reflects contents of that file share no longer than the set time ago
- pricing model
    - pay for only what is used
    - charged for data transferred, type & amount of storage used, and requests made
        - request: data written: up to max $125/gateway/mo. $0.01/GB to write data, first 100 GB free 
        - transfer out from storage gateway to on-prem gateway has tiers of charges 
    - if storage gateway is an appliance, also charge associated with the physical appliance 
- Planning & Designing Deployment
    - deploy on prem on VM, with hardware device, or create with AMI for EC2 
    - once deployed, must communicate w storage gateway service for mgmt & data movement
        - connectivity via
            - public endpoint
            - VPC endpoint
            - FIPS (federal information processing standard) 140-2 compliant endpoint for GovCloud workloads 
        - provide IP address that's either public or accessible w/n current network
        - generate activation key w console & use instead of ip
    - recommended to allocate at least 20% of exisitng file store size as cache storage 
        - max supported size of local cache for gateway on VM is 64TiB
    - Adding file shares
        - after gateway is created & activated
        - can be mounted to linux or windows
        - each file share is associated w a unique S3 bucket or unique prefix on the same bucket 
            - file metadata (such as ownership) stored as S3 object metadata cannot be mapped across different protocols 
    - additional considerations
        - verify region supports file gateway
        - file gateway stores file data in any region where buckets for file shares are located 
        - s3 storage classes
            - s3 standard
            - s3 intelligent-tiering
            - s3 standard-IA
            - s3 one zone-IA
            - recommended to write directly to standard & use lifecycle policies to transition object to other classes 
- working w S3 file gateway
    - read, writes, updates
        - all data xfer'd b/w gateway and AWS is encrypted using SSL. Data xfers done through HTTPS. Objects encrypted w SSE-S3 and optionally SSE-KMS
        - reads
            - when client reads from gateway, uses NFS or SMB
            - for client read requests, cache is checked, and if not there, then fetched from S3 using 'byte-range gets' to better use bandwidth 
            - storage gateway service retrieves data from S3 & sends to gateway appliance. Appliance receives data, stores in local cache & provides to client 
        - writes
            - when client writes to gateway, stored locally. Data is compressed asynchronously, changed data uploaded 
            - uses multipart uploads to asynchronously update objects 
        - write updates
            - gateway uses multipart uploads and copy put, so only changed data is uploaded to S3, and data in the cloud is used to create new version of the object 
    - managing the gateway
        - adding file share
            - when created, there are no shares
            - max of 10 shares
            - each share needs to be connected to S3 bucket & given access via IAM role w trust policy
        - file share health: available | creating | updating | deleting | force_deleting | unavailable
        - file share actions 
            - after creating file share, can't change bucket, access point, or VPC endpoint settings
            - can edit S3 storage class, edit name, export as read-write or read-only, refresh cache and more 
                - s3 request pricing applies to cache refresh
            - what can be changed is dependent on whether using NFS or SMB
            - storage gateway can invoke EventBridge when file operations are completed 
        - multi-writer best practices (when multiple gateways or fileshares write to same S3 bucket)
            - configure S3 bucket so that only one fie share can write to it 
                - multiple readers is fine, but it's recommended to have multiple clients write to file gateway, and one write gateway communicate w S3 
            - configure separate, unique object prefix for each file share to avoid writing to the same objects simulatenously 
        - upload notification: notifies when all files written to file share are uploaded to S3 
- secure & monitor gateway
    - use IAM identities & policies 
        - storage gateway file share has an IAM role attached, and bucket may have bucket policy as well
            - needs a trust policy (e.g., "Action": "sts:AssumeRole") and permissions policy
            - api actions: ActivateGateway | AddCache | CreateNFSFileShare | ListFileShares | RefreshCache
    - protecting your data 
        - data in the cache is not encrypted
        - data encrypted in transit and in the cloud 
        - NFS
            - on the file share, can limit access to specific clients/networks by IP
            - permit read-only or read-write access
            - activate user permission squashing
        - SMB
            - limit access for AD users only or authenticating guest access 
                - if guest access authentication is configured, POSIX is used for permissions
            - setting file share visibility to read-only or read-write
            - controlling file/dir access by POSIX (portable operating system interface), or fine grained permissions using Windows ACLs
        - access to S3
            - write IAM user policies to govern access
            - write specific bucket policies
            - use S3 block public access to limit public access
            - restrict access to specific actions w S3 object lock & setting guess MIME type and requester pays
    - monitoring and alerting
        - CloudWatch to monitor health of gateway & file shares
        - EventBridge to notify when file ops are done
        - AWS CloudTrail to track user activity

# Amazon S3 Volume Gateway
- provides cloud-backed iSCSI block storage volumes to on-prem apps
- when activated, gateway storage volumes created
    - mapped to on-prem DAS (direct attached storage) or SAN (storage attached network) disks
    - start w new disks or disks already in use
- modes
    - cached volume
        - primary data stored in S3 & retain frequently accessed data locally in cache. Reduces cost for primary (on-prem) storage and gives low-latency access to frequently used data
        - data that is modified is moved to upload buffer, and prepared for asynchronous upload to S3 
        - requires cache storage & upload buffer
    - stored volume
        - store data locally & asynchonously back up point-in-time snapshots to S3 
        - provides durable & affordable offsite backups 
        - only available w on-prem host platform options 
        - only requires an upload buffer (no cache storage)
- to prepare data for upload, gateway stores incoming data in upload buffer, a staging area
    - use DAS or SAN disks for working storage
    - gateway uploads data from upload buffer via SSL to storage gateway in AWS, then stored in S3 as EBS snapshots 
- snapshots of storage volumes can be taken
    - incremental; only capture changes since last snapshot
    - can initiate on schedule or one-time basis
    - to recover, restore EBS snapshot to on-prem gateway storage volume 
- decision factors to use volume gateway
    - want to centrally manage & automate volume snapshots
    - volume sizes steady & predictable
        - volume resizing not supported
            - to decrease storage, new gateway must be created & data migrated to new gateway
            - to increase storage, add new disks to the gateway; can't expand disks previously allocated 
    - are processes that require reading all data on the entire volume used? E.g. virus scans 
        - use stored mode
    - not able to access snapshot data using S3 console or APIs
        - cannot access via S3 console; must use storage gateway or EBS management console 
- pricing
    - pay only for what is used
    - charged based on amount of data xferred out of AWS, type & amount of storage used, requests made
        - if storage gateway deployed via hardware appliance, also cost of appliance 
## Planning & Designing a Volume Gateway Deployment
- Overview
    - Cached volume
        - use S3 as primary data storage
        - frequently accessed data is cached on premesis
        - can range from 1 GiB to 32 TiB
        - each gateway configured for cached volumes can support up to 32 volumes for max 1,024 TiB (1 PiB)
        - use cases
            - custom file shares
                - good for apps w large dataset, but app only needs low-latency access to a subset of that data at a given time
            - migrating app data into S3 & transition to EC2
    - Stored Volume 
        - data stored locally & backed up asyncronously to S3
        - take one-time or scheduled snapshots
        - good for durable off-site backups
            - quick to restore EBS snapshot to EC2
        - range from 1 GiB to 16 TiB
        - each gateway configured for stored volumes supports to up 32 volumes for max volume storage of 512 TiB (.5 PiB)
        - can only do stored volume on prem
        - use cases
            - block storage backups
                - e.g. dataset has large working set that can't be split up & need low latency access all the time
            - migrations or phased migrations
            - cloud-based disaster recovery
    - deploying
        - on prem
            - download VM from AWS 
            - purchase hardware appliance 
        - on AWS
            - deploy storage gateway AMI in EC2
                - supported for cached volume
        - gateway appliance sizing
            - determine number of total volumes & capacity needed
            - estimate app & workload volume
                - minimum requirement
                    - cache storage: minimum of 150 GiB for cached volume
                    - upload buffer storage: minimum of 150 GiB for cached & stored volume
                    - best practice to increase performance is to allocate multiple local disks for cache storage w at least 150 GiB storage
                        - no cache storage allocated for stored volumes 
    - connectivity b/w gateway appliance & service 
        - public endpoint
        - VPC endpoint
        - FIPS 140-2 compliant endpoint for GovCloud 
    - network considerations
        - requires the following ports: 443 | 80 | 53 (DNS) | 22 | 123 (NTP) | 3260 (iSCSI)
    - adding volumes to gateway appliance
        - cache volume
            - use console to provision storage volumes backed by S3 
            - can also use API or SDK
            - mount those storage volumes to on-prem app servers as iSCSI devices
        - stored volume
            - map to on-prem DAS or SAN disks
                - start w new disks or disks already holding data
            - mount those storage volumes to on-prem app servers as iSCSI devices 
    - additional considerations 
        - region
            - verify region supports volume gateway; must be selected before deploying gateway
            - EBS snapshots initially stored in region where volume gateway is created  
                - can back up snapshots in other regions using cross-region copy in AWS backup
        - recommended to configure CHAP (Challenge-Handshake Authentication Protocol) to access iSCSI
## Working with Volume Gateway
- reads, writes, and updates 
    - reads
        - cached volumes
            - if data is in cache volume, served locally & no latency
            - if not incache, compressed data from S3 retreived & sent to gateway appliance
            - appliance decompresses, stored in local cache, & provides to app (known as read-through cache - app always requests data from cache)
        - stored volumes 
            - 100% of data on volumes stored locally. all reads come directly from virtual appliance or SAN on local disk
    - writes
        - cached volumes
            - data written to local volume cache in native format
            - gateway compresses and encrypts data as it moves from cache to upload buffer
            - upload buffer provided so that performance not impacted when reading/writing to cache
            - as write happens, data goes into cache and is write-back; get local ack quickly w low latency & high thruput
            - from upload buffer, data xfer'd to AWS
        - stored volumes
            - happen directly to the disk & immediately acknowledged
            - when snapshot created, data will then be moved to AWS
            - data compressed & encrypted when moved to upload buffer
            - data securely xfer'd to S3
    - working w iSCSI initators 
        - iSCSI (small computer system interface)
            - facilitate data xfer's over intranets & manage storage over long distances
            - applications (called initiators) can send SCSI commands to storage devices (called targets) on remote servers 
        - for volume gateway, iSCSI targets are volumes
            - part of that includes connecting to those targets, customizing iSCSI settings, and configuring CHAP
        - iSCSI initiator
            - client component; sends requests to target
            - can be implmemented in hardware or software; storage gateway only supports software initiators 
        - iSCSI target
            - server component that receives and responds to requests from initators 
            - each volume exposed as target
            - connect one initiator to each target 
- working w volumes
    - once gateway is running, add volumes to associate w gateway
    - volume recovery point
        - point in time in which all data of volume is consistent and can be snapshotted or cloned
    - recovery options
        - volume clone
            - cloning from exisitng volume is faster & more cost effective that creating EBS snapshots
        - EBS snapshots
    - expanding volume size
        - automatic resizing not supported
            - create a snapshot of the volume desired & use snapshot to create new volume of a larger size
            - use cached volume to be expanded to clone a new a new volume of a larger size 
- managing gateway  
    - managing bandwidth for gateway
        - by default, gateway has no rate limits on upload or download
        - can throttle upload throughput from gateway to AWS or download throughput from AWS to gateway
            - gateway console > actions > edit bandwith rate limit 
- optimizing cost & performance 
    - volume storage is billed only for amount of data stored on volume, not size of created volume
        - best practice: delete older volumes & snapshots that are no longer needed
            - deleting a volume does not automatically delete associated snapshots 
            - to reduce storage costs, delete snapshots > 30 days old 
    - performance recommendations
        - optimize iSCSI settings to achieve higher I/O
            - choose 256 KiB for MaxReceiveDataSegmentLength and FirstBurstLength and 1 MiB for MaxBurstLength
        - back gateway virtual disks w separate physical disks
            - e.g., don't provision local disks for upload buffer & cache storage that use same physical disk
        - change volume config
            - if adding more volumes to gateway reduces throughput, consider adding volumes to another gateway
## Secure and Monitor Volume Gateway
- granting permissions
    - resource owner: AWS account of the principal entity that authenticates the request that creates the resource 
    - to grant resource access, gateway assumes a role that's associated w IAM policy that grants the access 
- protecting data
    - data encyprted in transit & in AWS cloud
    - data on gateway cache not encrypted
    - all data transfers b/w gateway appliance & AWS storage encrypted w SSL/TLS
    - all data stored by Storage Gateway in S3 is encrypted with SSE-S3 (default) or SSE-KMS depending on config
    - EBS snapshots encrypted at rest w AES-256
    - CHAP authentication (an iSCSI protocol)
        - authentacte b/w gateway & iSCSI initators 
        - protects against man-in-the-middle and playback attacks by periodically verifying identity of iSCSI initator
        - to set up, configure in Storage Gateway console & iSCSI initiator software used to connec to target 
        - uses mutual CHAP

# AWS Network Connectivity Options
## Connectivity Concepts
- General Concepts 
    - multi-tier architecture
        - often presentation > application > data
    - multi-vpc architecture 
    - High availability: reducing or managing failures and minimzing downtime w redundant/parallel components & eliminate single points of failure (SPOF)
    - Hybrid network
## Understanding AWS Network Service Offerings 
- VPC endpoints 
    - privately connect VPC to supported AWS services & VPC endpoint services
    - resources w/n VPC do not require public IP address to communiate w resources outside VPC
    - traffic b/w VPC & service do no leave Amazon network 
    - does not require gateway, NAT, VPN, or direct connect
    - Types of VPC Endpoints
        - Gateway VPC Endpoints 
            - target specific IP routes in a route table using prefix lists
                - there are specific prefixes for the supported services 
                    - e.g. com.amazonaws.<region>.dynamodb, com.amazonaws.region.s3
                    - list is used for traffic destined to DynamoDB or S3
        - interface endpoints
            - powered by AWS PrivateLink
            - is an ENI w private IP address from subnet CIDR range
            - serves as entry point for traffic destined to supported AWS service or VPC endpoint service
        - gateway load balancer (GLB) endpoint
            - is an ENI w private IP address from subnet CIDR range
            - entry point to intercept rtraffic and route to a service configured using GLB endpoint as target for route on route table
                - e.g., used for security inspection 
            - powered by AWS PrivateLink
    - pricing
        - charged for each hour VPC endpoint is provisioned in each AZ & each GB processed thru VPC endpoint
- AWS PrivateLink
    - private connection b/w VPCs & AWS services
    - provides secure usage w/n AWS network 
        - services establish TCP connection b/w service provider's VPC and service consumer's VPC
    - avoids exposing traffic to public internet 
    - benefits
        - security
            - provides VPCs secure & scalable way to privately connect to AWS hosted services
            - uses private IP addresses and SGs w/n VPC
        - simplification
            - removes need to permit public IPs w IGWs, NAT gateways, or firewalls
            - does not require route table modification
            - not necessary to establish IGW, VPC peering, or transit VPC
        - capabilities
            - gives on-prem network private access to AWS thru Direct Connect 
            - can make services available to other accounts or VPCs that are accessed securely as private endpoints
                - use PrivateLink w NLB to route traffic to app, and clients can connect to any hosted app 
    - considerations
        - doesn't support IPv6
        - communicates w NLB that serves traffic to instances w/n subnet, so all IP addresses logged by app will be private IP addresses from NLB; will not see IP of customer or service consumer 
        - Activate Proxy Protocol v2 on NLB for NLB to send add'l connection info like source & destination (may require changed to app)
        - endpoint services cannot be tagged 
        - endpoint DNS does not resolve outside of VPC
        - endpoint services available in the region which they are created & can be accessed in remote regions using inter-region VPC peering
    - DNS
        - endpoint-specific regional DNS hostname
            - automatically generated & includes unique endpoint identifier, service identifier, region, and `vpce.amazonaws.com` in its name
        - zonal-specific DNS hostname
            - can generate zonal-specific DNS hostname for each AZ that endpoint is available in. Hostname includes AZ in its name
            - support cross-zone load balancing
            - regional data xfer chanrges might apply for data xferred b/w AZs
        - private DNS hostname
            - can use private DNS hostname to alias the auto created regional-specific or zonal-specifc DNS names
- VPC peering
    - networking connection b/w two VPCs that allows private traffic routing
    - benefits
        - HA
            - it's not gateway or reliant on physical hardware; simply helps facilitate xfer of data 
        - inter-region VPC peering: establish peering relationships b/w VPCs across regions 
            - uses IP addresses w/o requiring gateways, VPN connections, or other network appliances
            - traffic encrypted w no SPOF or bandwidth bottleneck & never transverses public internet 
        - can peer b/w different accounts 
    - scenarios 
        - full sharing of resources b/w all VPCs
            - requires *EACH* VPC to be connected to another VPC, one at a time, and all those links have to be maintained
        - partial sharing of centralized resources
            - e.g., VPCs communicate w one security VPC, and each one only has to maintain that one connection 
    - non-valid peering configs
        - overlapping CIDR blocks
        - transitive peering
            - peering from A to B and B to C does not grant A to C
        - edge-to-edge routing thru gateway or private connection
            - if any VPC in peering has the following, can't extend peering to that connection
                - VPN connection or direct connection to corporate network
                - internet connection thru IGW
                - internet connection thru NAT
                - gateway VPC endpoint to AWS service
    - pricing
        - no charge for setting up or running. Data xfer is charged per GB for send & receive, regardless of AZs involved 
- AWS Direct Connect (DC)
    - private, reliable connection from AWS to physical facility
    - fully integrated & redundant service 
    - offers consistent performance w reduced bandwidth cost 
    - speed
        - offers physical connections of 1, 10, 100 Gbps
            - 100 Gbps available only in select locations 
        - supports Link Aggregation Control Protocl (LACP), which allows multiple dedicated physical connections to be grouped into Link Aggregation Groups (LAGs)
            - max 2 100 Gbps connections in a LAG, or 4 connections with port speeds <100 GbPs
            - each connection counts toward overall connection limit for region
            - all connections in the LAG must terminate at same Direct Connect endpoint 
    - network requirements
        - network must be co-located w existing Direct Connect location
            - customer deploys router & supporting networking equipment to location w physical uplink to AWS
            - router at DC location connected to AWS router using cross-connect 
        - working w Direct Connect partner
            - DC partner provides physical equipment to connect to AWS router at Partner's physical location
            - physical link used to config DC service to connect on-prem to AWS
        - working w independent service provider to connect to Direct Connect 
        - technical requirements
            - network must use single-mode fiber
                - 1000BASE-LX transceiver for 1-Gb ethernet
                - 10GBASE-LR transceiver for 10-Gb ethernet
                - 100GBASE-LR4 for 100 Gb
            - port auto-negotiation must be deactivated
                - port speed & full-duplex mode must be configured manually
            - 802.1Q VLAN encapsulation must be supported across entire connection
            - device must support BGP and BGP MD5 authentication
            - (optional) can configure Bidirectional Forwarding Detection (BFD) on network. Asynchronous BFD automatically activated for DC virtual interfaces, but not on-prem until configured on physical device
    - LOA-CFA (Letter of Authorization and Connecting Facility Assignment)
        - must be signed for each new physical connection individually from the DC console 
        - used to show operater of facility hosting AWS router that request approved to use AWS router
    - Virtual interface types
        - private virtual interface
            - permits traffic to be routed to any VPC resource in the same private IP space as the virutal interface
        - public virtual interface
            - permits traffic to be routed to any VPC or AWS regional resouce w a public IP address in same region
        - transit virtual interface
            - permits traffic to be routed to any VPC or AWS regional resouce routable thru AWS Transit Gateway in same regaion
    - pricing
        - pay for what is used; no minimum fee
            - port hour
                - determined by connection type (dedicated or hosted) and capacity
            - outbound data xfer
                - charged per GB
- AWS Site-to-Site VPN
    - enables securely connect on-prem network to Amazon VPC (e.g. branch office site)
    - based on IPsec technology
    - each tunnel terminates in different AZ on AWS side, but must terminate on the same customer gateway on the customer side 
    - customer gateway
        - resource created & configured in AWS that represents on-prem gateway
        - contains info about type of routing, BGP, ASN, and other info
    - customer gateway device
        - physical device or software app on customer side of connection
    - virtual private gateway
        - VPN concentrator on AWS side
    - transit gateway
        - transit hub used to interconnect VPCs and on-prem networks
        - have to use this or VPG
    - limitations
        - IPv6 partially supported; dualstack thru separate tunnels for inner traffic
        - IPv6 for outer tunnel connections not supported
        - does not support Maximum Transmission Unit (MTU discovery)
            - greatest MTU available on inside tunnel interface is 1,399 bytes
        - throughput of all AWS S2S VPN connections limited
            - when terminating on VPG, only one tunnel of pair can be active & carries max of 1.25 Gbps, which is closer to 1 Gbps IRL
            - when terminate on TG, both tunnels in pair can be active & carry max of 2.5 Gbps. IRL, closer to 2 Gbps
            - each flow (e.g., TCP stream) will be limited to max 1.25 Gbps (IRL 1 Gbps)
        - max packets per second (PPS) per tunnel: 140,000
        - AWS S2S VPN terminating on TG supports equal-cost multi-path routing (ECMP) and multi-exit discriminator (MED) across tunnels in same different connections
            - ECMP only supported with TG VPN config
            - MED is used to ID primary tunnel for S2S VPN connections that use BGP
            - BFD not supported on AWS S2S VPN; it IS supported on DC
        - uses public IPv4 addresses & require public virtual interface to transport over DC
            - AWS S2S VPN over private DC not yet available 
        - for globally distributed apps, accelerated S2S VPN option provides connection to global AWS backbone thru AWS Global Accelerator
            - Cannot use global S2S VPN w DC public virtual interface 
        - recommended to avoid overlapping CIDR blocks 
    - pricing
        - AWS S2S VPN
            - connection per hour (varies by region)
            - data xfer out charges (based on EC2 on-demand pricing)
        - Accelerated S2S VPN
            - connection per hour (varies by region)
            - data xfer out charges (based on EC2 on-demand pricing)
            - hourly charges for two AWS Global Accelerators per VPN connection
            - data xfer out premium (DT-Premium) fees
                - depends on source region & edge location (based on Global Accelerator pricing)
- AWS Client VPN
    - enabled secure connection for users to AWS or on-prem networks (e.g. remote employees)
    - managed client-based VPN based on openVPN technology
    - client VPN endpoint
        - VPN admin creates & configs Client VPN endpoint in AWS
        - admin controls networks & resources that are accessible when connection is made 
    - client VPN application
        - software app used to connect to client VPN endpoint & establish connection
    - client VPN endpoint config file
        - provided to user by VPN admin
        - info about endpoint & certs required to establish connection
        - loaded into chosen VPN client app 
    - limitations
        - supports IPv4 only
        - Security Assertion Markup Language (SAML) 2.0-based federated authentication only works w AWS provided client v1.2.0 or later
        - SAML integration w AWS SSO requires workaround. Better implementation being worked on
        - client CIDR ranges must be at least /22 and not greater than /12
        - client VPN endpoint does not support subnet associations in a dedicated tenancy VPC
        - client VPN *NOT* compliant with FIPS
        - client CIDR range cannot overlap w local CIDR of VPC where associated subnet is located
            - cannot overlap any routes manually added to client VPN endpoint route table
        - portion of addresses in client CIDR range is used to support availability of client VPN endpoint; recommended to use CIDR block that contains twice the number of required IP addresses
        - client CIDR range cannot be changed after creating client VPN endpoint
        - subnets associated w client VPN endpoint must be in same VPC
        - cannot associate multiple subnets from same AZ w client VPN endpoint
        - ACM certs not supported w mutual auth since private key can't be extracted
            - can use ACM server as server-side cert
            - generate client-side cert when user has key or use AWS Certificate Manager Private Certificate Authority (ACM PCA) that gives private keys 
                - if customer authenticating based on AD or SAML, can use gneral ACM-generated cert b/c only server cert is required 
    - monitoring
        - cloudwatch
        - can do basic posture assessment w Lambda
    - pricing
        - charged for number of active client connections per hour & number of subnets associated to Client VPN per hour
        - any client connection that is less than an hour is also prorated for the hour
- AWS Transit Gateway
    - HA and scalable service that provides interconnectivity b/w VPCs and on-prem network
    - w/n region, provides option for consolidating & centrally managing routing b/w VPCs w hub-and-spoke network architecture 
    - b/w regions, supports inter-region peering w other transit gateways 
        - facilitates routing network traffic b/w VPCs of different regions over AWS global backbone 
        - traffic not routed over the internet 
    - key concepts
        - supported attachments
            - one or more VPCs
            - compatible Software-Defined Wide Area Network (SD-WAN) appliance
            - DC gateway
            - peering connection w another TG
            - VPN connection to a TG
        - TG MTU
            - 8,500 bytes for
                - VPC connections
                - DC connections
                - connects w other TGs
                - peering connections
            - 1,500 bytes for
                - VPN connections 
        - TG route table
            - has default route table 
            - can optionally have add'l route table
            - includes dynamic & static routes that decide next hop based on destination IP of packet
            - target of routes can be any TG attachment
        - Associations
            - each attachment associated w exactly one RT
            - each RT can be associated w zero or many attachments 
        - route propogation 
            - VPC, VPN connection, or DC gateway can dynamically propagate routes to TG RT
                - DC attachment: routes are propogated to TG RT by default
                - VPC: must create static routes to send traffic to TG
                - VPN or DC gateway: routes propagated from TF to on-prem router using BGP
                - peering attachment: must create static route in TG RT to point to peering attachment
    - inter-regional peering
        - two types of peering connection for VPCs in different regions: VPC peering, TG peering
            - both are one-to-one
            - TG are simpler network design & consolidated mgmt 
    - pricing
        - charges for number of connections per hour and per GB of data processed
            - see AWS TG pricing page 
- peering vs TG:
    - peering is cheaper and less latency
    - TG is less complex 

# Data Analytics on AWS
## AWS Data Analytics Stack Portfolio
- Four layers of AWS data analytics portfolio
    - layer 1 (bottom): Data movement
        - AWS DMS, Snow family, Kinesis Data Firehose, Kinesis Data Streams, Managed streaming for Apache Kafka
    - layer 2: data lake infrastructure & management
        - S3/glacier, Lake Formation, Glue
    - layer 3: analytics
        - Redshift, EMR (Spark & Presto), Glue (Spark & Python), Athena, Elasticsearch Service, Kinesis Data Analytics
    - layer 4 (top): data visualization, engagement, and ML
        - Data Exchange, QuickSight, Pinpoint, SageMaker, Comprehend, Polly, Lex, Rekognition, Translate
## Data Migration Options 
- AWS data migration options
    - move to the cloud w Direct Connect, Storage Gateway, S3 transfer acceleration, Snowball, Kinesis Data Firehose, DMS
- Modernizing a data warehouse w Redshift 
    - data warehouse aka OLAP (online analytical processing) defined:
        - store data that has strict data types that conform to a schema w rows & columns in denormalized (adding precomputed redundant data to an otherwise normalized RDB) table 
        - central repository of curated data from different sources
        - benefits
            - better decision-making
            - consolidated data from many sources
            - improved dat quality, consistency, accuracy
            - access to historical intelligence
            - improved performance 
    - problems w on-prem data warehousing
        - difficult to scale
        - hardware slow to procure
        - complex upgrades are the norm
        - high overhead admin costs
        - expensive licensing/support and limited number of users/data
        - propriety formats that don't support newer open data formats lead to data silos 
        - data not cataloged & unreliable quality
        - difficulty in integration
    - Redshift: fully managed cloud data warehouse
        - key features
            - optimized for high performance
            - support for open file formats
            - petabyte-scale capability
            - support for complex queries and analytics, w data visualization tools
            - secure end-to-end encryption + certified copmliance
            - 99.9% SLA
            - based on open source Postgre DB
            - cost efficient
        - performance features
            - massively parallel processing
                - takes large job, breaks into smaller tasks & distributes tasks to multiple compute nodes
            - columnar storage 
                - data from each column stored together + data can be accessed faster w/o scanning & sorting all other columns & improves performance
            - shared-nothing architecture
                - independent & resilient nodes w/o dependencies; improves scalability
        - architecture
            - managed service; cluster runs in AWS-managed internal VPC
            - leader node resides in customers VPC & provides access endpoint
            - consists of leader node + one or more compute nodes 
            - client app only interacts w leader node using JDBC (Java DB Connectivity) or ODBC (Open DB Connectivity) protocols
        - resizing approaches
            - elastic resize
                - existing cluster modified to add or remove nodes in two stages
                    - stage 1: cluster temp unavailable while metadata migrated, usually completes in minutes. Session connections held & queries remain queued
                    - stage 2: session connections reinstated, queries resume; cluster available for R/W operations
            - classic resize
                - one+ hours to complete, depending on data size. Stream all data from original cluster to new cluster. During resize, OG cluster in read-only mode. Customer charged for only one cluster 
        - three available nodes
            - RA3 (redshift analytics)
                - latest & recommended node type
                - flexible; customers grow & pay for compute & storage independently
                - uses combo of SSD for hot & S3 for cold data; efficient & high performance
                - based on AWS Nitro System w high-speed networking for fast access to S3
            - DC2 (dense computing)
                - uses high-performance local SSD storage
            - DS2 (dense storage)
                - comes w local hard disk drive storage; considered legacy & generally not recommended
        - management interfaces
            - console, CLI, SDKs, SQL tools, redshift query API
        - differentiating features
            - federated query
                - integrate queries on live data in RDS & Aurora PostgreSQL + quries w redshift & Amazon data lake 
                - intelligent optimizers pushes & distributes some computing directly into remove operational DBs to reduce data moved over network
                - benefits
                    - query operational DBs directly, apply transformations on the fly, load data into target tables w/o complex ETL pipelines
            - lake house architecture 
                - run queries in redshift to join w other data in S3 data lake w/o moving or transforming data
                    - customers can query open file formats directly in S3 & write it back
                - use SQL statements to combine & process data across data stores
                - run quries on live data in operationsl DBs w/o data loading + ETL pipelines 
        - migrating to Redshift 
            - AWS provides WQF (workload qualification framework) service, which uses AWS SCT (schema conversion tool) to collect info from legacy data warehouse & generate reports:
                - workload assessment based on: complexity, size of migration effort, technology used
                - recommendation on migration strategies
                - step-by-step instructions for migration
                - assessment of migration effort based on team size + member roles 
            - AWS SCT
                - redshift extracts data thru local migration agents
                - data optimized for redshift & saved in local files
                - files loaded to S3 (via network or snow family) and then to redshift 
    - Data Lakes 
        - one motivating factor for customers to migrate to cloud & modernize data analytics is to have all their data in one place & in a single repository
        - defined
            - stores all structured, semi-structured, unstructured, and binary data at unlimited scale
            - holds curated & raw data
            - uses AWS data analytics tools for analytics
            - increase pace of innovation by extracting insights from data
            - enabled more organizational agility
            - reduces cost & delivers results w predictive analytics and ML
        - secure data lakes
            - S3 provides scalable, durable object storage & is foundational to building data lake
## Data services
- AWS Glue
    - fully-managed, ETL (extract, transform, load) service
    - simplifies and automates difficult & time-consuming data discovery, conversion, mapping + job scheduling tasks
    - pointed to data stored in data lake or other data stores
    - Glue discovers data & stores metadata in AWS Glue Data Catalogue
        - once catalogued, data searchable + available for ETL
    - four main components
        - AWS Glue Data Catalogue
            - Hive metastore comp0atible w enhanced functionality
            - crawlers automatically extract metadata + create tables
            - integrates w Athena, EMR, + more
        - Job Authoring
            - generates ETL code
            - build on open frameworks (Python, Scala, Apache Spark)
            - dev-centric editing, debug + sharing
        - Running jobs
            - run on serverless Spark platofmr
            - flexible scheduling, job monitoring, alerting
        - Job workflow
            - orchestrate triggers, crawlers, jobs
            - author + monitor flows w integrated alerting
    - Glue Data Catalogue
        - unified metadata repository across RDBs, AWS RDS, Redshift lake house, S3
        - benefits
            - single view into data, regardless of storage location
            - automatically classify data in central, searchable list
            - track data evolution using schema versioning
            - query data w Athena or Redshift Spectrum 
                - use catalogue w standard SQL language to query any data store
            - hive metastore (central repo of Apache Hive metadata) compatible
    - AWS Glue crawlers
        - automatically build + sync customer's data catalog
        - scans sources like RDS, redshift, DDB, data lakes & constructs data catalog using pre-built classifiers for open source formats & data types, like JSON, CSV, Parquet 
        - performs following tasks
            - classifies data to determine format, schema, associated properties of raw data
                - custom classifiers can be created
            - group data into tables or partitions based on crawler heuristics
            - write metadata to AWS Glue Data Catalog that's referenced by analytics tools to discover, understand, query data
            - customers configure how crawler adds, updates + delete tables & partitions
            - can schedule on demand or regularly at specific interval
- AWS Data Exchange 
    - helps engineers subscribe to third-party data to reduce integration issues 
    - benefits
        - find diverse data in one place
            - 1k+ data products, 80+ data providers 
            - includes free datasets + subscription datasets 
        - analyze data
            - dl of copy of data to S3
            - combine, analyze + model w existing data 
        - access third-part data
            - streamlined access to data
            - minimize legal reviews & negotiations 
            - immediate access upon subscribing 
- Amazon Athena
    - interactive query services that can analyze data in S3 using std SQL
    - to use, point to data in S3, define schema, and query w std SQL
    - most results delivered w/n seconds
    - features
        - no setup costs
        - pay per query
            - save 30-90% on query costs w compression
        - Open source
            - SQL interface, JDBC/ODBC drivers, multiple formats, compression types, complex joins + data types
        - streamlined
            - serverless; zero infra, zero admin, integrated w QuickSight
- AWS Lake Formation
    - simplifies process of building secure data lake
    - automates steps to set up data lake w few steps in unified process (reduces process from months [on-prem] to days [cloud])
    - four stages
        - ingest + organize
            - automates create dataing lake + data ingestion
        - secure + control
            - sets up fine-grained access control & data governance
        - collaborate + use
            - search + data discovery w Data Catalog metadata
            - to protect data, access checked against set policies
        - monitor + audit
            - based on data access & governance policies, alert notifications raised on policy violation + logged
    - many integrated services: Glue, Athena, Redshift Spectrum, QuickSight Enterprise Edition, EMR
        - builds on AWS Glue 
    - uses Blueprints, AWS Glue, and security services to build
        - fully managed data lake
            - integrates w AWS security, storage, analysis, and ML services
        - powerful data cleansing
            - can optionally cleanse data to remove dupes, fill in missing values, link records, remove data errors
        - automated data ingestion
            - supports batch + stream data ingestion into S3
        - centralized access control
            - defines access permissions, including table, row, and column-level control + encryption policies for data at rest & in transit
            - can use varieety of AWS analytic & ML services to access data lake. All access secured, governed, auditable
        - customers create workflow based on predefined Blueprints which is used by CloudFormation to create Glue workflow
    - benefits
        - builds data lake + customers can use AWS data analytics tools w secure data to extract value faster
        - all users can run analytics based on access, at any scale securely & cost-effectively
        - comprehensive set of integrated tools enables every user equally
        - centralized management of fine-grained permissions helps security officers
        - simplified ingest & cleaning helps data engineers
        - cost effective, durable storage + global replication ability
- QuickSight
    - cloud powered BI service that can deliver insights to everyone in org
    - benefits
        - scalable
            - from 10 to 10k+ users in minutes
            - pay-as-you-go
        - pay for use
            - pay monthly or annually
            - pay-per-session pricing; no up-front costs 
        - serverless & fully managed 
            - create dashboards quickly 
            - deploy globally w/o provisioning servers
        - fully integrated 
            - secure + private access to AWS data
            - integrated w S3 data lake & IAM
            - can use API & embed in apps & share w others 
    - serverless data lakes + analytics 
        - GUI can be used to analyze and visualize data
        - uses Athena & Redshift spectrum among others, depending on Glue Data Catalogue
## Data Analytics Solution 3: Amazon Kinesis
- Streaming data
    - data generated continuously by thousands of data source that send data records simultaneously and in small sizes 
    - examples
        - log files from customers using mobile or web apps
        - ecommerce purchase
        - in-game player activity
        - info from social networks
        - financial trading floors
        - geospace services
        - telemetry from connected devices or instrumentation in data centers 
    - must be processed sequentially & incrementally on record-by-record basis or over sliding time windows 
    - uses for real-time analytics 
        - value of data, in most cases, diminishes over time
        - anything over a few minutes is considered batch-oriented analytics 
    - enabling real-time analytics 
        - source
            - devices and/or apps that produce real-time data at high velocity
        - stream ingestion
            - data from 10s of 1000s of data sources must be ingested & written to single stream
            - requires ability to scale a solution to keep up
        - stream storage
            - data stored in order received for set duration & can be replayed indefinitely during this time
        - stream processing
            - records in storage processed by real-time apps to generate real-time analytics, run real-time ETL & deliver data to destination
        - destination
            - output of stream processing sent to + consumed by another AWS service or an app
    - challenges
        - difficult to set up
        - difficult to achieve HA
        - error prone + complex to manage
        - tricky to scale
        - integration requires deevelopment
        - expensive to maintain
- AWS streaming data solutions
    - Amazon Kinesis
        - family of analytics services for streaming data
        - services straightforward to set up, configure and use
        - HA & durable, fully managed & scalable
        - core services
            - Kinesis Data Streams
                - capture & store data streams
            - Kinesis Data Firehose
                - load streaming data into streams, data lakes, warehouses
            - Kinesis Data Analytics
                - analyze data streams in real-time
            - Amazon Managed Streaming for Apache Kafka
                - fully managed service for Apache Kafka
    - Amazon Kinesis Data Streams (KDS, DS)
        - massively scalable, highly durable data ingestion + processing service for real-time streaming
        - features 
            - collected data available w/n 70 ms
            - real-time analytics w dashboards, anomaly detection, dynamic pricing
            - data synchronously replicated across 3 AZs in region & stored up to 7 days
            - serverless; scale to handle MB or TB each hour & 1,000s to 1,000,000s of PutRecords per second
            - no upfront cost; pay-as-you-go pricing
        - how it works
            - capture and send data in real-time -> ingest & store data streams for processing > build custom, real-time apps (using K Data Analytics, Spark on EMR, EC2, Lambda) -> analyze stream data w BI tools 
            - sources
        - architecture
            - data producers: ec2 instances, client, mobile client, traditional server
            - data stream: process & store data 
                - shard: temporary holding space to accumulate data until processed
                    - base throughput unit of Kinesis data stream
                    - contains sequence of records ordered by arrival time
                    - ingestion throughput of 1,000 records each second or 1 MB each second
                    - data record
                        - unit of data stored in Kinesis stream
                    - data blob
                        - data of interest that producer adds to stream
                    - partiion key
                        - meaningful identifier for click stream or timestamp
                    - sequence #
                        - unique identifier 
            - data consumer 
                - Kinesis Data Firehouse, ec2 instance, Kinesis Data anlytics
                - can consume or store or pass on to other storage space
        - provisioning
            - when provisioned, must calculate how many shards their app needs
            - volume & velocity of data records must be estimated
            - shard count can be increased
            - choice of partition key impacts number of shards 
    - Kinesis Data Firehose (KDF, DF)
        - reliably loads streaming data into data lakes & stores + analytics tools 
        - can capture, transform, and load streaming data into S3, Redshift, Elasticsearch Service, Splunk
        - enables near real-time analytics w existing BI tools & dashboards already in use
        - fully managed, scales automatically to match throughput of customer data 
        - how it works
            - capture & send data -> prepare & load data (w KDF) continuously to selected destinations -> durably store for analytics (use S3, Redshift, Elasticsearch, Splunk) -> analyze stream data w analytics tools 
    - Data Streams vs Firehose
        - processing time: DS on ms scale; DF on second time-scale; 60-900 seconds 
        - stream storage + duration
            - DS: shards; default 24h up to 7 days
            - DF: max buffer size 128 MB, max time 900 seconds
        - data transformation & conversion
            - DS: none
            - DF: Lambda, Glue
        - data producer
            - same; Kinesis Agent, apps using Kinesis Producer Library (KPL), SDK for Java, Cloudwatch Logs + Events, AWS IoT
        - data consumer
            - DS: Lambda, Kinesis Data Analytics, DF, Apps using Kinesis Client Library (KCL) and SDK for Java
            - DF: Lambda, Kinesis Data Analytics, DF, Apps using KCL + SDK for Java, S3, Redshift, Amazon ES, Splunk, 
        - data compression
            - DS: none
            - DF: gzip, Snappy, Zip, or none
        - when to use
            - Data Stream
                - data stream app that ingests massive amounts of data
                - millisecond response time
                - massively scalable
                - data storage from hours to days
                - e.g. real-time gaming
            - Data Firehose 
                - data stream app that requires near-real-time response in seconds
                - performing data augmentation, transformation, or compression
                - must save data to S3, Redshift, ES, Splunk, or send to K Data Analytics
                - e.g. log analytics captured in small batch nodes 
    - Amazon Kinesis Data Analytics (KDA, DA)
        - efficient way to analyze streaming data
        - gain actionable insights
        - respond to business + customer needs in real-time
        - quickly build SQL queries or Java apps
        - takes care of everything required to run real-time apps nearly continuously
        - how it works
            - input
                - streaming source for app
                - map input to in-app data stream so data flows from source into in-app data stream
                - data then can be processed from in-app data stream using app code
            - app code
                - series of Java operators or SQL statements
                - app can split in-app data stream into multiple streams & apply different logic to separate streams
            - output
                - where app code write after completing processing 
                - e.g stored in S3 or Redshift 
            - capture streaming data w MSK, KDS, KDF, other sources -> query & analyze streaming data w KDA -> send process data to analytics tools to create alerts & respond in real-time

## Data Analytics Solution 4: Data Governance
- challenges of data in data lakes
    - securing data
    - auditing data usage
    - managing data access
    - safeguarding senitive data & PII
    - maintaining regulations and mandates 
- Amazon Macie
    - helps protect PII data
    - full managed data security + privacy service
    - uses ML + pattern matching to discover & protect sensitive data in AWS
    - Enable (in console or w API call) -> automatically generate inventory of S3 bucket & details on the bucket security -> analyzes bucket w ML to discover sensitive info -> generates findings & sends to CloudWatch Events 
        - Provides inventory of buckets, including
            - which ones aren't encrypted
            - which ones are publicly accessible
            - which ones are shared w AWS accounts
    - search & filter findings in console
        - send to CloudWatch Events for integration w existing workflow & event mgmt systems
    - data identifiers
        - financial
        - personal
        - national
        - medical
        - credentials & secrets 
    - DIDL (de-identified data lake) on AWS
        - architectural approach to help orgs use data while reducing risks of managing it
        - benefits
            - reduce risk: remove PII before entering data lake
            - understand data: create data catalogue of lake
            - reduce compliance costs: automate discovery, classification, de-identification, and monitoring of data in org
            - turn data into an asset: enable broader set of governed analytic & ML use cases
        - how it works (masking PII data)
            - masking obscures part or all of a column value
            - fake values replace column values
                - not the same as encryption or hashing; simply replaces real info with fake info (e.g., instead of 'Just talked to Carlos Salazar' that same entry shows 'Just talked to Jane Doe')
## Data Analytics Solutions 5: Insights & Monetization w ML
- ML fueled by data, and data lakes are great sources
- also erquires high-performance compute power + flexible independent scaling of storage and compute
- AWS analytics tools can process raw data into ML-friendly data & store in lake to be processed by ML
- ML requires
    - more data: collect all types of data
    - flexibility: define schema during analysis
    - scalability: storage/compute need to scale well and independently
    - data transformation & processing: process w/o moving data
    - security: network, identity, encryption, compliance 
- Amazon SageMaker
    - fully managed service that provides dev the ability to build, train, deploy ML models
    - runs a managed Jupyter Notebook app; notebooks used for common problems 
    - SageMaker Debugger: unified interface for analysis, debugging, discovery, alerts
    - SageMaker Experiements: one-click training, hyperparameter optimization
    - streamlines deploying ML model to prod
## Technical Engagement
### Data Flywheel
1. Move and store data in the cloud
2. Move and manage all workloads in the cloud
3. Build data-driven apps (3 and 4 are almost a fork between 2 and 5, working parallel, not necessarily serially)
4. Analyze with data lake architectures
5. Innovate with Machine Learning 

### Six-Phase Strategy for Implementing Data Analytics Solutions 
- Data analytics in the cloud assessment 
    - assess where customer is at in the journey; e.g., if they need to be convinced of AWS usefulness, available services, etc
- Use case identification
- Architecture and data migration
- Proof of concept delivery
    - POC best practices
        - common pitfalls
            - incorrectly scoped project
                - if project not properly scoped, there's a lack of clarity on what POC should deliver
                - results oversold & effort underestimated
                - mitigation:
                    - create well-documented SOW (statement of work); all parties should agree before work begins
                    - demonstrate value by solving customer's key pain points in specific timeframe + budget w/o overcommitting 
                    - offer workshop services to deepend relationship + trust w customer's IT team

            - lack of skills to complete POC
                - if customer's technical teams are lacking the right skills, can be challenging to complete poc
                - mitigation:
                    - work with AWS PDM (partner development manager) to educate technical teams 
    - Delivering successful POC
        - clearly identify the end goal
            - use case should address customer's key challenges w/n specific period; there shouldn't be an exepctation of 'silver bullet'
        - aim for optimal architecture up-front 
            - use tools that align with current skillset if a steep learning curve would otherwise be required 
        - ID criteria + metrics that measure success
            - all parties must agree on clear criteria
    - Success factors
        - get committment from customer & c-level sponsor to keep team moving
        - create well-scoped SOW
        - use PSA (partner solutions architect) to review POC + SOW
        - have well-defined deliverables
        - identify all AWS services for the POC
        - use agile dev processes
        - track risks & have contingency plan
        - use AWS PDM/PSM to ID POC funding programs 
- App tuning + optimization
- Migration from POC to products 

### Well-Architected Review using the Analytics Lens
- 10 design principles 
    - automate data ingestion to handle big data 
    - design ingestion for failures and duplicates 
    - preserve original source data 
        - keeping raw data helps repeat ETL process in event of failure 
    - describe data w metadata
        - capture metadata during ETL process; document + automate
    - establish data lineage 
        - track data origin + flow b/w data systems 
    - use right ETL tool for the job
        - streamline workflow b/w source + destination 
    - orchestrate ETL workflows 
        - automate ETL
        - chain ETL jobs to keep workflow smooth & track + debug failures 
    - tier storage appropriately
        - based on format & access pattern
    - secure, protect, and manage entire analytics pipeline
    - design for scalable + reliable analytics pipelines 
        - this way, volume or velocity of data doesn't impact prod pipelines 

# AWS Security Fundamentals
- Design principles
    - implment strong identity foundation (least privilege)
    - enable traceability in real-time
    - apply security at all layers (defense-in-depth)
    - automate security best practices with IaC
    - protect data in transit and at rest with encryption + access control
    - enforce the principle of least privilege 
    - prepare for security events; establish an incedent management process 
- AWS Shared Responsibility Model
    - AWS is responsible for protecting global infrastructure that runs all services offered in AWS Cloud ('security OF the cloud')
    - customer is responsible for securing data, OS, networks, platforms, & all resources created in AWS cloud ('security IN the cloud')
        - responsible for CIA (confidentiality, integrity, availability) triad 
## Security OF the Cloud
- Data Center Security
- Compliance and Governance 
    - AWS communicates about security + control env by
        - obtaining industry certs + third party attestations
        - publish info about AWS security + control practices in whitepapers & website content
        - provide certs, reports, other docs to AWS customers under NDA (as required)
        - provide security features + enablers + compliance playbook & mapping docs for compliance programs 
    - running workloads on AWS does not automatically make workload compliant 
        - only have to certify customer apps + arch to ensure compliance; all underlying AWS stuff is already compliant 
    - AWS Artifact
        - no-cost, self-serve portal for access to security + compliance reports + online agreements 
## Security IN the Cloud
- Identity and Access Mgmt
    - AWS IAM
        - careful mgmt of access creds is foundation of securing resources in cloud 
    - additional services for IAM
        - AWS Secrets Manager
        - AWS IAM Identity Center (Formerly Single Sign-On)
        - AWS STS
        - AWS Directory Service
        - AWS Organizations 
    - Amazon Cognito
        - add user sign-up, sign-in + access controls to web + mobile apps 
        - define roles & map users to diff roles so app can only access authorized resources 
        - sign-in directly w Cognito or via third party
- Detective Controls 
    - AWS CloudTrail
        - track API call history for changes to AWS resources 
        - route events + info reflecting potentially unwanted changes into a proper workflow 
        - can trigger automated actions for remediation
    - Auditing on AWS
        - S3 Server Access Logs 
            - contain details about requests: 
                - request type
                - resources requested
                - date/time of request 
        - ELB Access Logs 
            - detailed info about each request sent to LB
                - client IP address
                - latencies
                - server responses
            - analyze traffic patterns + troubleshoot issues 
        - Cloudwatch Logs + Events 
            - Logs
                - monitor + troubleshoot OS + apps in AWS env
                - monitor logs for specific phrases, patterns, values
            - Events
                - track overall activity level of CW rule collection 
        - VPC Flow Logs     
            - ensure network access config'd properly
            - capture info about IP traffic going in/out of network interfaces + subnets 
        - CloudTrail
            - history of API calls to account made via console, CLI, SDKs, or other AWS services 
    - Additional AWS Services for Detective Controls
        - GuardDuty
            - intelligent threat detection service
            - continuously monitor + protect AWS acct's and workloads 
            - uses ML to detect anomalies
            - monitors unusual API calls + direct threats like compromised instances 
        - Trusted Advisor
            - inspects AWS env + make recommendations based on best practices to save money, improve performance, tighten security gaps 
        - Security Hub
            - single view of high-priority security alerts + compliance status across AWS accts 
    - AWS Config
        - continuous monitoring + assessment service to detect non-compliant configs in almost real-time
- Infrastructure Protection 
    - defense in depth: subnet routing, NACLs, SGs
    - AWS Systems Manager
        - features
            - automation
            - inventory
            - patch manager
            - parameter store
            - run command
            - session manager 
    - AWS Firewall Manager
        - centrally configure + manage WAF rules across accounts and apps 
        - can bring new apps + resources into compliance w common set of security rules
    - AWS Direct Connect
        - establish dedicated + secure network connection from on-prem to AWS 
    - AWS CloudFormation
        - automate & simplify task of repeatedly creating & deploying AWS resources 
        - ensure all security + compliance controls deployed along w new env 
    - Amazon Inspector
        - automated security assessment service
        - continually scans + assess apps for vulnerabilities or deviations from best practices
        - findings aggregated in Inspector consle, routed to Security Hub, pushed thru EventBridge 
- Data Protection
    - data at rest
        - client side encryption (encrypted before sent)
        - server-side encryption (encrypted after received)
    - protection in transit
        - AWS services provide HTTPS endpoints using TLS for end to end encryption when communicating w AWS APIs
        - use AWS to generate, deploy, manage public & private certs for TLS
        - use IPSec w VPN connectivity into AWS to encrypt traffic 
    - AWS CloudHSM
        - computing device that processes cryptographic operations & provides secure storage for cryptographic keys
    - Amazon S3 Glacier
        - storage for cold data
        - enforce compliance controls for individual vaults w vault lock policy
    - AWS Certification Manager
        - handles complexity of creating + managing public SSL/TLS certs for AWS-based websites + apps
        - can issue private SSL/TLS X.509 certs that ID users & devices
    - Amazon Macie
        - uses ML to discover, classify, protect sensitive data in AWS
    - AWS KMS
        - managed service to create + control keys used in data encryption 
- Incident Response
    - AWS Step Functions
        - coordinate multiple AWS services into serverless workflows to build + update apps quickly
        - can 'stitch' together services like Lambda and CFN to respond to incident in the cloud 
- DDoS Mitigation 
    - services for out-of-region protection
        - Amazon Route 53
            - HA + scalable DNS service
            - hosted at numerous AWS edge locations
            - able to absorb large amounts of DDoS traffic 
        - Amazon CloudFront
            - CDN (content delivery network) service used to deliver data to end users
            - only accepts HTTPS + HTTP well-formed connections to prevent DDoS
        - AWS Shield
            - managed DDoS protection service that safeguards web apps on AWS
    - AWS WAF
        - protect web apps from common web exploits 
# Exam Readiness: AWS Certified Solutions Architect Associate 
## Module 1: Design Resilient Architectures 
- reliable, resilient storage 
    - instance store 
        - ephemeral
        - only certain instances
        - fixed capacities based on instance type
        - app-level durability
        - use for caching + storing temp data that's replicated elsewhere 
    - EBS (Elastic Block Store)
        - attaches to one instances at a time
        - multiple can be attached to a single instance
        - can use RAID w multiple EBS volumes 
        - supports encryption + snapshots
        - some can configure IOPS
        - persists beyond lifetime of instance 
        - SSD volumes
            - higher IOPS
            - best for random access 
            - types
                - gp2 (general)
                - io1 (provisioned IOPS)
                    - scale read/write
        - HDD volumes
            - best for sequential access; larger blocks require less IOPS
            - throughput optimized more expensive than cold storage 
    - EFS 
        - shared storage; multiple instances can access same volume
        - capacity elastic up & down
        - supports NFS 4.0 & 4.1
        - compatible w Linux NOT Windows 
        - mount points created in specific VPC
            - can only be attached to one VPC at a time
    - S3
        - distributed system with strong consistency for new objects
            - eventually consistent with updated objects 
                - might get old object if reading updated object quickly after the update
        - standard, standard-IA
        - encryption
            - data at rest: SSE-S3, SSE-KMS, SSE-C
            - data in transit: HTTPS (uploads + downloads)
        - versioning
            - previous versions of file remain available after file is updated/deleted
        - access: IAM policies, bucket policies, access control lists
        - multi-part upload: large files uploaded in smaller parts
    - Glacier
        - data backup + archive storage 
        - bulk: ~12 hours
        - expedited: $$$ ~5 minute retrieve 
        - encrypts by default
        - ideal to set up lifecycle policies in S3 buckets 
        - regionally available 
- design decoupling mechanisms using AWS services
    - e.g. use SQS Qs between external server and internal operations 
    - load balancer, autoscaling 
- design multi-tier architecture solution
    - naturally decoupled
- design HA and/or fault tolerant 
    - failure should be treated as an operational event, not an exceptional event
    - "everything fails all the time"
    - examples: fault tolerant requires more beef; if app expects 4 instances to run efficiently, needs 4 instances per AZ. HA requires less instances; if app expects 4 instances to run efficiently, 4 instances split in 2 AZs is sufficient 
- CloudFormation
    - declarative programming language for deploying AWS resources 
    - create, update, delete set of resources as single unit 
    - not region-specific; requires maps to specify different AMIs/region
- Lambda
    - fully managed compute service that runs stateless code in response to event or time-based interval
    - run code w/o needing to manage compute services 
- test axioms (important to remember)
    - single AZ will never be the right answer
    - using AWS managed services should always be preferred
    - high availability: system will be up and can failover in event of failure
    - fault tolerance: [higher requirement] service must silently provide the same level of service to the customer
    - expect that everything will fail at some point and design accordingly 
## Module 2: Design Performant Architectures
- choose performant storage & DBs
    - EBS volumes
        - SSD
            - general purpose max: 16TiB size, 10k IOPS/volume, 160MiBs thruput
            - provisioned IOPS max: 16TiB size, 32k IOPS/volume, 500MiBs thruput
        - HDD
            - throughput-optimized max: 16TiB size, 500 IOPS/volume, 500MiBs thruput
            - cold max: 16TiB size, 250 IOPS/volume, 250MiBs thruput
    - S3
        - offload static content to S3 to improve web server performance 
        - virtual hosted-style URLs: bucket name as part of the domain name
            - e.g. http://bucket.s3-aws-region.amazonaws.com
        - path-style url: bucket first part of the path
            - e.g. https://s3-ap-northeast-1.amazonaws.com/[bucket name]/[key]
        - can specify region in url in path (optional)
        - buckets are always tied to a region
        - bucket names are globally unique 
        - pricing
            - GBs/mo storage
            - transfer out of region (xfer into region free)
            - per PUT, COPY, POST, GET
        - classes
            - S3 standard: cheaper retrieval, higher storage cost
            - S3 IA: cheaper storage, higher retrieval cost 
        - lifecycle policies
            - move cold data to IA storage 
    - Databases
        - RDS
            - managed relational DB
            - use cases
                - complex transactions/quries
                - medium-to-high query write rate
                - no more than a single worker node/shard
                - high durablility
                - scale via larger instance or read replicas 
                    - can only scale read ability, NOT writing 
                    - read replicas supported on MySQL, MariaDB, PostgreSQL, Aurora
        - DynamoDB
            - table grows as data need is required; throughput defined
                - Read Capacity Unit (one item up to 4KB)
                    - one strongly consistent read/s
                    - two eventuallly consistent reads/s
                - Write Capacity Unit (one item up to 1KB)
                    - one write/s
            - use cases
                - massive read/write rages (e.g. 150k+ write/s)
                - sharding, horizontal auto-scaling
                - simple GET/PUT requests/queries
        - Redshift (data warehouse w SQL interface)
            - analytical vs transactional queries 
- apply caching to improve performance
    - caching is good for data that isn't expected to change much or doesn't need to be fresh 
    - CloudFront (managed CDN)
        - cache at web level
        - cache content closer to users 
        - user requests routed to most optimal edge location. non-cached content retrieved from origin and cached at edge location
        - can be used for static & dynamic content 
            - benefit for dynamic content is that response comes via AWS backbone instead of public internet 
        - origins: S3, EC2, ELB, HTTP servers 
        - integrates w WAF & Shield std + advanced 
    - Elasticache
        - cache what would otherwise be pulled from DB backend 
        - managed memcached
            - simplier & easier setup
        - managed redis
            - more powerful
    - scaling
        - vertical
            - change in instance specifications
        - horizontal (scaling in and out)
            - increasing number of available instances
    - autoscaling
        - easiest way to horizontally scale 
        - components
            - launch configuration
                - specifies instance size + AMI type
            - autoscaling group
                - references launch config
                - specifies min, max, desired size of ASG
                - may reference ELB
                - health check type
            - autoscaling policy
                - use CloudWatch to determine scale in and out
                    - monitor CPU, network, queue size 
- design solutions for elasticity & scalability
- test axioms
    - if data is unstructured, S3 is generally the best storage solution
    - use caching strategically to improve performance
    - know when and why to use Auto Scaling
    - choose the instance and DB type that makes the most sense for your workload and performance need 

### Module 3: Specify Secure Applications and Architectures
- overview
    - shared responsibility model (security OF vs IN the cloud)
        - managed services move the line of responsibility of AWS higher in to the 'IN' category
    - least privilege 
        - IAM
            - roles: temporary identities 
            - policies: permissions of accessible AWS resources
                - attach to users, roles, groups
            - integrate w AD & other ID providers w federation
        - identities
            - IAM users
            - roles
            - federation
            - web identity federation 
                - use roles w authentication from Open ID provider & STS
- secure application tiers
- secure data tier
    - data in transit
        - in/out of aws
        - w/n aws
        - use SSL over web, VPN for IPsec, IPsec over direct connect, import/export w snow family
        - data sent to AWS API uses SSL
    - data at rest
        - stored in S3 or EBS
            - private by default, requires AWS creds for access
            - server-side encryption
                - SSE-S3, SSE-KMS, SSE-C
                - generally easier and more performant 
            - client-side encryption
                - CSE-KMS, CSE-C
        - manage keys
            - KMS
            - CloudHSM
                - dedicated appliance that customer manages keys w/n 
- networking infrastructure for single VPC app 
    - subnet recommendatiosn
        - public subnets contain entry to IGW
        - private subnets DO NOT have an entry to IGW
            - not directly accessible from internet
            - use NAT gateway for outbound access
            - use bastion/jump box for inbound access
        - NACL: subnet level
            - allow and deny traffic
            - stateless
        - SG
            - only allow traffic (can't explicitly deny)
            - stateful
            - apply to network interfaces 
            - can be config'd to allow access to other interface or SG; don't need to only use IP address to allow access 
            - use to control traffic into, out of, b/w resources 
            - can span subnets, not regions
- test axioms
    - lock down root user
    - security groups only allow. network ACLs allow explicit deny
    - prefer IAM Roles to access keys

### Module 4: Design Cost-optimized Architectures
- overview
    - pay as you go
    - pay less when you reserve
    - pay even less per unit by using more (volume discount)
- design cost-optimized storage
- design cost-optimized compute 
    - considerations of EC2 cost
        - clock hour of server time
        - machine config
        - machine purchase type (pricing models)
            - reserved instances
                - standard 
                - convertible   
                    - can be changed to different sizes/types
                - scheduled 
            - spot instances
                - use spare compute
                - lose instance if price goes above target 
                    - hybernate: put instance to sleep if spot price goes above target 
                    - reserve up to 6 hours on the spot market 
            
        - number of instances
        - load balancing
        - detailed monitoring
        - auto scaling
        - elastic IP addresses
        - OS + software packages
        - tenancy
        - storage class
        - storage
            - could use S3
            - EBS considerations
                - volumes
                - IOPS
                - snapshots
                - data transfer costs of snapshots 
        - requests
        - data transfer 
- serverless architectures 
    - don't pay for idle time
    - lambda: pay per invocation 
    - s3 for static files
    - DDB for storing state
    - API gateway to attach REST API to lambda
    - CloudFront to avoid fetching data 
        - no data xfer charge for data b/w CF & S3
        - cost based on how widely traffic is to be shared 
- test axioms
    - if it's going to be on, reserve it
    - any unused CPU time is a waste of money 
    - use the most cost-effective data storage service and class
    - detremine the most cost-effective EC2 pricing model and instance type for each workload 

## Module 5: Operationally-excellent Architectures
- enable operational excellence
    - ability to run and monitor systems to deliver business value + continually improve supporting processes & procedures
    - key practices
        - prepare
        - operate
        - evolve 
    - best practices
        - perform ops w code
        - annotate documentation
            - should be live representation of what is deployed
        - make frequent, small, reversible changes
        - refine operations procedures frequently
        - anticipate failure
        - learn from all operational failures   
            - evolve & harden systems
    - AWS services supporting operational excellence
        - AWS Config
        - AWS CloudFormation
        - AWS Trusted Advisor
        - AWS Inspector
        - VPC Flow Logs
        - AWS CloudTrail
        - Amazon CloudWatch
- test axioms
    - IAM roles are easier and safe thean keys and passwords
    - monitor metrics across the system
    - automate responses to metrics where appropriate
    - provide alerts for anomalous conditions 

# AWS Machine Learning (ML)
## Module 4: Data Collection, Integration, Preparation, Visualization, and Analysis 
- data challenges
    - requires lots of data
    - requires clean data 
    - >50% of project time spent gathering, cleansing, + visualizing data 
    - structured data
        - prefined format, e.g., RDBs, .csv files
    - semi-structured data
        - text-based, e.g., email
    - unstructured data
        - data not in pre-defined format
        - word documents, phone calls, text messages, pictures, etc 
- build the data platform 
    - S3, Athena, EMR, Glue, QuickSight, Lake Formation, Redshift 
- preparation steps
    - data exploration + profiling
    - data formatting
    - data conversion
    - encoding (all data must be numeric for ML)(e.g. bools or numbers)
    - data cleaning
    - normalization/standardizations 
    - resampling (oversampling/undersampling)
- SageMaker Data Wrangler 
    - GUI to transform data
    - w single click, ingest data from various data sources & deploy data preparation workflows into prod
    - leverage built-in features to
        - transform data w data transformations (e.g. add missing data to standardize)
            - 300+ available data transformations 
        - understand data visually using visualization templates 
        - quick-model: train model from data wrangler
    - ability to 
        - bring custom transformation
        - estimate ML model accuracy 
- SageMaker Feature store
    - challenge in scaling ML productivity
        - long + tedious feature engineering
        - redundant feature pipelines
        - no sharing or discovery mechanism
        - troubleshooting overhead due to training inference skew
    - benefits
        - ingest features from many sources - including data wrangler
        - search + discover features easily
        - ensure feature consistency for training + inferences
        - standardize w single source of feature definition 
    - can be made in the GUI in SageMaker console, or programmatically 
- SageMaker Ground Truth
    - fully-managed data labelling service
    - quickly label training data + easily integrate human labelers  

## Module 8: Amazon SageMaker Built-in Algorithms 
- algorithms: standardized methods used to train models
- model: function that maps inputs to a set of predicted outcomes using algorithms
- types of algorithms (families of ML)
    - linear regression
    - logistic regression
    - clustering
    - vectorization
    - image classification
    - encoding and decoding
    - language processing 
- built-in algorithms
    - general purpose
        - numerical regression or classification
            - linear learner (supervised learner)
                - 'simple' problems
            - XGBoost (supervised learner)
                - classification regression + ranking
                - can combine (sample) multiple weaker algorithms
                - CPU-only, memory bound 
                - best for complex problems
            - K-Nearest Neighbors (k = choose the number)
                - meant for simpler problems 
        - recommendation
            - factorization machines 
                - e.g. recommend what user should look at
                - capture interaction b/w data sets
                - passes output to KNN
        - group entities based on data
            - k-means
                - find distinct groups w/n data
                - unsupervised 
        - detect anamolies in time series data 
            - RCF (random cut forest)
                - find anomalies and outliers 
                - unsupervised 
    - specific use cases
        - classify images or find objects in images (uses neural networks)
            - image classification
            - object detection
                - determine where an object is in an image 
            - semantic segmentation 
                - find borders of a thing w/n an image 
        - classify, encode, and transform text data (natural language processing)
            - sequence to sequence (seq2seq)
                - text or audio
                - generates sequence of tokens
                - use neural network to encode and then decode
            - NTM (neural topic model)(unsupervised)
            - LDA (latent dirichlet allocation)(unsupervised)
            - blazing text 
                - text classification
                - unsupervised: extract features from text
            - object2vec
                - word to vector 
        - reduce dimensions in datasets w high numbers of attributes
            - PCA (principle component analysis)
                - plug columns of dataset and break into components
                - unsupervised, but needs number of components to be provided 
        - predict future trends based on past history (time series)
            - deepAR forecasting
                - add variables to data (e.g. weather, tempurature) to make predictions
        - find usage patterns in network access logs
            - IP insights 

# Managing the Application Lifecycle in Amazon ECS
## Module 1: Preparing Your Environment to Run Containers
- Reviewing infrastructure requirements 
    - ECS terminology
        - cluster: logical grouping of tasks or services
            - tasks + services run on infrastructure registered to the cluster
        - service: allows running & maintaining specified number of tasks + determines how tasks are added & configured 
        - task: one or more running containers
            - instantiated from a task definition
            - task definition: describes one or more containers that for app
                - specified various parameters + can include container image to use, ports to expose, CPU/memory utilization, logging, and env vars 
    - compute infrastructure 
        - AWS Fargate
            - managed serverless compute engine for ECS containers
                - few management req's and handles security, patches, updates
            - can scale to a greater degree of precision to prevent paying for unneeded capacity
            - only pay for time containers are running 
        - EC2
            - pay only for underlying compute instances 
            - use cases: 
                - long-running + computationally demanding workloads
                - workloads w predictable utilization
                - workloads not supported by fargate
        - on-prem/hybrid 
            - use ECS Anywhere to run container workloads on-prem 
            - get same cluster management, workload scheduling & monitoring on-prem as in cloud
            - use cases:
                - workloads must reside in on-prem data centers for compliance req's
                - low-latency access to resources req'd
                - investments already made into on-prem infra 
    - network infrastructure 
        - offers three networking models 
            - awsvpc mode (supports Fargate + EC2)
                - ECS creates + manages ENI for each task + each task receives its own private IP address w/n VPC
                    - ENI separate from underlying host's ENI
            - host mode (EC2 only)
                - networking of container tied directly to underlying host that's running container
            - bridge mode (EC2 only)
                - use virtual network bridge to create layer b/w host + networking of container
                - create port mappings that remap a host port to container port 
        - ALB/NLB support 
            - cannot attach more than five target groups to a service using LBs
            - services using awsvpc network mode, and ip target type must be used on LB
- Infrastructure Deployment Options 
    - supported infra deployment tools 
        - CloudFormation
        - CDK (Cloud Development Kit)
            - software dev framework that defines cloud app resources using declarative model + programming languages
            - can be used to generate CFN templates 
        - AWS Copilot
            - CLI for launching + managing containerized apps, automating deployment pipelines, deploying app infra using service patterns 
            - can create LBs, VPCs, ECR registries
            - resource config'd based on opinionated best practices for simplified deployment 
        - AWS proton
            - fully managed delivery services for containerized apps 
            - includes tools for infra provisioning, code deployments, monitoring, updates
            - platform teams create stacks that define compute, networking, code pipeline, security, and monitoring resources
                - once stacks are published, devs select stacks to use to deploy their infra 
        - third-party tools (e.g. terraform, pulumi)
        - AWS App2Container 
            - CLI tool for converting .NET + java apps into containerized apps 
            - analyzes and builds inventory of apps running in VMs in on-prem or cloud envs
            - uses CFN to provision cloud infra + CI/CD pipelines to deploy containerized app 
- AWS CloudFormation
    - deploying infra w CFN
        - features
            - extensibility (supports deployment of third-party resources + modules)
            - cross-region + cross-account mgmt (able to share templates)
            - safety controls 
                - determines right ops to perform, provisions resources efficiently + automatically rolls back if errors encountered 
            - dependency mgmt (automatically manages dependencies b/w resources)
            - community support (easy to find support resources + CFN templates)
    - when to use CFN
        - provide granular control over infra 
        - requires in-depth familiarity w all aspects of env
        - ideal for orgs that want to actively manage IaC
- AWS CDK
    - declarative model to define + deploy cloud app resources
    - simplified creation of CFN templates
        - CDK generates CFN templates
        - provides default properties for resources based on Well-Architected Framework
    - constructs for generating infra
        - construct: define a resource or combination of resources including config details 
    - deploying infra + runtime code together 
    - concepts
        - app: project in CDK; holds stacks
        - stacks: groups of resources that comprise the app 
            - each stack in CDK app associated w an env
                - env is target AWS account/region where stack to be deployed 
        - constructs: represent AWS resources to app to define app infra 
            - L1 constructs
                - low-level constructs directly represent CFN resources
                - directly represent resources available to CFN
                - rarely used; need similar granularity as CFN template 
            - L2 constructs
                - represent AWS resources w a level of abstraction 
                - add convenience methods that make it simpler to work w resources 
            - AWS construct library
                - contains patterns (higher level constructs)
                - integrate resources from several services 
            - patterns for ECS
                - ALB, NLB, Q processing, scheduled tasks 
    - when to use
        - when orgs want to define + deploy infra w declarative model + common programming languages 
- AWS Copilot
    - CLI for managing entire lifecycle of app dev and deployment 
    - automates the following tasks
        - creating deployment envs
        - building infra for target env
        - conducting docker builds + pushing images to ECR
        - deploy containers to AWS fargate hosts in ECS 
        - monitor running services 
    - deploying infra
        - automatically deploys specific AWS resources to support different aspects of release process
        - application infrastructure 
            - for each region app is deployed into, KMS key and S3 bucket created
                - CodePipeline uses these resources to allow cross-region/account deployments 
                - all pipelines in app share the same resources 
        - environment infrastructure 
            - when env created, new VPC in two AZs created
            - ECS launches services in public subnets, but only available if LB deployed 
            - if domain name set up w R53, new subdomain name will be generated each time env is deployed
                - format: environment-name.app-name.your-domain.com
            - can also use ACM to provision SSL/TLS so ALB can use HTTPS
        - service infrastructure
            - service types
                - request-driven web service
                    - Runs on AWS App Runner
                        - managed service that creats + configs underlying infra of app
                        - scales container instances up/down to meet traffic need 
                    - best for apps where very little control over infra is desired 
                - load balanced web service
                    - copilot creates ECS service running tasks on Fargate. Creates ALB w/n env VPC for inbound traffic
                    - best for HTTP services w steady request volumes that need to access resources in VPC or require advanced config 
                - backend services 
                    - create preconfigured service w/o connectivity out of VPC
                    - will provision ECS running on fargate, but won't set internet-facing endpoints 
                - worker service    
                    - implements asynchronous service-to-service comms w publish-subscribe architecture 
                    - copilot deploys infra where microservices publish events to SNS topics & then consumed by worker service 
                    - copilot creates worker service on fargate, service-level SQSQ, dead letter Q for failed messages, and SNS topic
    - when to use
        - use cases that prioritize rapid deployment over customized infra 
        - only deploys fargate-backed clusters 
- AWS Proton
    - vending infrastructure w Proton
        - create infra templates & make them available to others in org
        - centralized infra team can use Proton to make a stack that defines everything needed to provision, deploy + monitor service
        - devs can use published stacks to automate infra provisioning 
    - concepts
        - templates: parameterized IaC + CI/CD templates deployed to create env + service infra
        - environment templates: templates to create shared resources where services can run, like VPC, cluster infra, LBs
        - service templates: templates to create workloads running in an env, like EC2 or fargate
        - service instance: instantiation of aservice inside deployed env 
    - when to use
        - org wanting to control cost + enforce compliance 

# Developing Serverless Solutions on AWS
## Module 8: Step Functions for Orchestration
- use step functions to build a state machine to orchestrate workflow
- helps keep lambda functions focused on business logic 
- terms
    - task: state that performs work
    - choice: state that provides branching logic
    - parallel: state that starts parallel branches
    - wait: state that pauses processing
    - map: state that iterates on an input array

# Amazon PinPoint 
- digital user engagement platform
- combines analytic data w data from record systems to get better view of customer behaviors 
- can ingest data from CRM/ERP/shopping systems

# Amazon Simple Workflow Service (SWF)
- managed workflow services that helps build, run, and scale apps that coordinate work across distributed components 
- fully-managed state tracker and manager for background tasks 
- supports parallel task completion
- benefits
    - logical separation
        - split b/w control flow of logic & units that have business logic 
    - reliable 
        - managed service + all built in benefits of that 
    - simple
        - replaces custom-coded pipelines and third-party apps
    - scalable
        - automatically scales w complexity of workflows 
    - flexible 
        - write app + coordination logic in any coding language
- key components 
    - domain: collection of related workflow
        - scope resources w/n account 
        - when register domain, have to define how long to keep workflow execution record 
    - workflow: collection of actions 
        - can't interact w workflows in other domains 
    - workflow starter: app that can start workflow execution
        - can pass add'l data to worker & get current state
        - needs client to start execution
        - can be local app, web app, CLI, console 
    - tasks
        - activity workers: do the work (e.g., check inventory, charge CC)
            - polls SWF for new tasks appropriate for the worker to perform
            - processes task to completion, then provides result to SWF
            - represents a single process or thread 
        - lambda task: executes function (and not SWF activity)
        - decision task: let decider know that something in workflow has changed
            - decision: data type that can contain info about next steps 
    - actions: tasks or workflow steps
    - deciders: implment logic
        - specify what should occur next in execution 
        - each decision task assigned per decider 
        - only one decision at once allowed 
    - workflow history: complete record of every event since workflow execution
    - deciders and activity workers communicate directly with SWF, not with each other 

# AWS SSM
- SSM agent required to be installed on all instances to be managed 
    - installed by default on AWS linux/windows instances
- benefits
    - run command
    - state manager
    - inventory manager 
    - maintenance window
        - applies patch baseline
    - patch manager
        - create patch baseline 
    - automation
        - use automation document to do things on instances 
    - parameter store 
        - config info like passwords, keys, db codes, licenses 
- services like CodeDeploy can use parameter store 

# AWS Transcribe
- ASR (automatic speech recognition) service 
- fully-managed and continually trained
- accurately transcribes low- and high- fidelity audio
    - can be used for subtitles & reviewing recording phone calls 
    - supports WAV, FLAC, MP3/4
- uses ML to add punctuation and formatting
- adds time stamp to each word to align with subtitles in a video
- able to recognize multiple speakers
- invoke service via API & easily able to transcribe & output to S3 
- transcription results are available for 90 days 

# AWS Translate 
- neural machine translation service using deep learning 
- any provided data encrypted at rest and transit 
- pay-as-you-go
- encoder reads source one sentence at a time and uses semantic representation to get the gist of the sentence, and uses an attention mechanism to understand phrases of speech then decoder outputs a word at a time in target language 
- for CLI, can provide text as an argument or as a .json file 

# Amazon Lex
- powers Amazon Alexa
- service for building conversational interfaces into any app using voice & text 
- provides deep learning & natural language understanding to understand and respond sensibly to a human 
- 4 stage process
    - save
    - build master language model (MLM)
    - test (in console)
    - publish
- publish to chat services directly from Lex console 

# Amazon Polly
- translate text information into life-like speech 
- part of AI services from Amazon
- helps increase accessibility 
- 24 languages, 48 voices total available 
- SSML (Speech Synthesis Markup Language)
    - define how bot should speak
    - change accent, speech rate, and lots of other 
- able to interpret abbreviations based on context (e.g. St. for Saint or St. for Street)

# Amazon ECR
- fully-managed container repository 
    - HA & access controlled w IAM
    - stores images in S3 
- image = container 'blueprint'

# Designing Event-Driven Architectures
- serverless: abstracting computing infra to the point of having no responsibilities for the servers on which code runs; not paying for server idle time 
- microservices: event-driven
    - use events to communicate b/w and invoke decoupled services 
- synchronous: think sequential 
- asynchronous: decoupled; message amounts and orders persisted w SQS Q
- client polling: common way to get status info on long-running transaction 
    - can result in a lot of unnecessary work + increased cost by having the service consistently polled 
    - advantages: 
        - convenient to replace a synchronous flow
    - disadvantages:
        - adds add'l latency to consistency of data to client
        - unnecessary work + increased cost for requests + responses when nothing has changed 
- web hooks: user-defined HTTP callbacks 
    - trust clients: own both sides of system + can create secure integration b/w them
    - untrusted client: receive webhook info thru registration process or part of data submission to API
    - e.g. can use SNS to set up HTTP subscribe + notifies client using webhook
        - added benefit, can model retries + exponential backoff until success 
    - downside: add'l complexity since client needs to host web endpoint 
- WebSockets w AWS AppSync 
    - open standard used to create persistent connection b/w client + backing service
    - allows bidirectional communication 
    - runs on GraphQL; newer standard than REST
    - ideal for streaming data or when data might yield more than one response 
- serverless data processing patterns 
    - processing w Kinesis
        - ingest + process large volumes of data in near-realtime 
        - producers add data records onto stream + consumers get records & process them 
            - lambda can be a consumer
            - streams can be consumers of other streams
            - create consumer apps using KCL (Kinesis Consumer Library)
            - create producers using KPL (Kinesis Producer Library)
        - KDS [stream]: scale horitzonally by adding shards
            - each shard has unique ID sequence of data record
            - data records include partition key + data blob
                - P key used to group data by shard w/n stream
            - shard can write 1,000 records/sec up to max of 1 MB/s
            - for reads, shard can support 5 transactions/s, up to 10k records & up to max of 2 MB/s
            - increase thruput by adding shards or use aggregation to increase number of records sent per API call
            - can write custom consumers
            - guarantees order deliver of data records as exactly-once delivery 
            - preserves sequence of records; if e.g. Lambda fails on one record in batch, entire batch (and associate shard) is blocked until failure is resolved 
            - can have multiple consumers & types of consumers 
            - limit of five consumers/shard 
            - enhanced fan-out consumer can be added
                - gets dedicated thruput
                - supports more than five consumers
                - add'l cost for each enhanced fan-out consumer 
        - KDF [firehose]
            - config data producers to send data to KDF & it automatically delivers data to specified destination
            - can also transform data before delivering it 
            - specify the amount of data or time period to buffer data
            - Kinesis creates the number of required shards; NOT customer defined 
            - specific, limited targets suported 
            - when Lambda associated w KDF stream, KDF tries invocation three times then skips that batch of records & records in processing_failed folder in S3
        - KDA [analytics]
            - write SQL statements or Apache Flink apps & upload them into KDA app to perform analysis on the data in the stream
            - supports using lambda to preprocess data before SQL executes 
            - input sources can be either KDF or KDS & KDA can output to those types of streams
            - can also use KDA as event source for lambda 
    - SNS
        - data pattern of messaging instead of streaming 
        - uses publication/subscription (pub/sub) model
        - single published message can have multiple consumers 
        - subscribers 
            - email addresses
            - HTTP endpoints
            - lambda functions
            - SQS Qs
        - decouples publishers and subscribers 
        - huge amount of topics & subscriptions allowed
            - 100k topics, 12.5m subs per topic 
            - able to kick off large number of processes w one SNS message 
        - message filtering can be done on key:value pairs 
    - event bridge 
        - events are observable: multiple services can listen to events, without a game of 'phone tag' through multiple services 
        - event bus splits up what downstream systems need to hear certain events 
- Failure Management in Event-Driven Architectures 
    - failure management in code
        - capture and retry a call whenever possible + handle errors gracefully when calls fail 
        - failure management in functions 
            - lambda logging library can send errors to CloudWatch logs 
        - error handling for synchronous and asynchronous events 
            - function error: Lambda successfully hands off event to function, but function errors/times out before completing
            - invocation error: request rejected before function receives it 
            - sync invocation: no retries built in
                - code has to be written to handle errors & retries
            - async invocation
                - w async event sources, lambda has built-in retry behaviors 
                - lambda tries two more times by default
                    - retry value can be b/w 0 and 2
                - lambda retries invocation for up to 6 hours by default
                    - decrease by using max age of event setting 
                    - use dead-letter Q to handle events that are too old 
        - error handling for stream based events (e.g. KDA, DDB streams)
            - streams need to maintain record order per shard
            - manage failures
                - bisect batch on function error: split failing batch into two and retry each separately
                - maximum retry attempts/record age: limit number or duration of retries on failed batch
                - on-failure destination: send failed records to SNS topic or SQSQ
            - there is a chance records can be processed multiple times, so function must handle idempotency and can't assume only-once record processing
        - failed-event destinations 
            - on-failure destination defined for async + streaming event sources
            - advantages
                - contains additional data
                - more flexbility to change or modify failure behaviors 
        - error handling w SQS as event source
            - best practice to set up dead-letter Q on source Q to process failed messages 
            - SQSQ timeout needs to be greater than the timeout of the lambda process
                - best practice: SQS should have a 6x greater timeout than the lambda timeout 
        - error handling summary by execution model
            - API gateway (sync event source)
                - has a 30 second timeout; error returned if Lambda function doesn't respond in 30 seconds 
                - no built-in retries
                - error handling: generate the SDK from API stage + use provided backoff and retry mechanisms 
            - SNS (async event source)
                - no timeouts; doesn't have to wait for response from function execution
                - built-in retries (max total of three attempts)
                - max event age 6 hours by default 
                - SNS has unique retry behaviors: 3 immediate tries, 2 tries at 1s apart, 10 tries, backing off from 1s to 20s, and 100,000 tries at 20s apart 
                - error handling: use lambda destionations OnFailure option to send messages to destination for processing or move to dead-letter Q
            - KDS (polling stream as event source)
                - retention period 24 hours by default
                    - increase retention period for a cost
                - by default, lambda retries failing batch until retention period expires 
                - error handling: OnFailure destination w lambda 
                - use BisectBatchOnFunctionError to tell Lambda to split failed batch into two batches & keep splitting batches to isolate bad records 
            - SQSQ (polling queue as event source)
                - timeout should be 6x greater than timeout set for function
                - use maxReceiveCount on queue's policy to limit number of times Lambda will retry to process failed execution
                - error handling: write functions to delete each message as it is successfully processed + move failed messages to DLQ
    - Failure Management Across Application
        - step functions for failure management 
            - features
                - errors & retries via looping
                    - tasks + parallel states have field named retry 
                - try/catch/finally logic
                - Amazon States Language provides predefined set of well-known errors 
                - retry and catch fields for error handling
                    - each catcher can handle multiple errors
                - states.ALL: wildcard that matches any error name 
            - can troubleshoot in the console, and the steps are created visually
            - SAGA pattern
                - manage failures where each step w/n larger business transaction includes compensating transactions that undo the changes made by its predecessors 
        - failure management with dead letter queues (DLQ)
            - SNS or SQS can be DLQs for lambda functions
            - create Q or topic first, then reference it in function config
            - lambda execution role must have perms to write to Q or topic 
            - DLQ on lambda
                - configured as part of function
                - sends messages to DLQ after 2 built-in retries
            - DLQ w SQSQ
                - DLQ part of Q policy
                - policy describes how many retries before item moved to DLQ
            - both lambda & SQSQs need a mechanism to redrive messages back to original source for processing 
        - AWS event work pipelines
            - prebuilt apps, available in AWS SAR (Serverless Application Repository) to be used in serverless app
            - Event Replay Pipeline buffers events from provided SNS topic into SQSQ so events can be replayed to another pipeline in DR scenario 
        - Distributed tracing w AWS X-Ray
            - tool that can be enabled for any lambda function, API gateway stage, or SNS
            - service graph gives visual representation of what's occuring at every step of the serverless workflow 
            - subsegments
                - provide more granular visibility
            - annotations
                - key:value pairs that group traces for app-specific ops 
        - synchronous event sources do not have built-in retries for failed or throttled requests 

# Architecting Serverless Applications 
## Migrating to Serverless 
- the key to thinking serverless is thinking in terms of patterns and applications, not in individual functions or resources 
- migration patterns
    - leapfrog: straight from on-prem to re-writing apps and going straight to serverless cloud arch 
    - organic: more lift-and-shift style
        - existing apps kept intact w some limited rewrites to container services
        - devs experiment w Lambda in low-risk scenarios. As confidence is built, use serverless for more tasks 
        - get buy-in for a long-term committment to modernizing apps & pilot a workload 
    - strangler: incrementally and systematically decomposes monolithic apps by creating APIs and event-driven components that eventually replace components of legacy app (most common)
        - new feature branches can be serverless first 
        - use canary deployments to test new components & easily fallback 
- domain-driven design
    - what does this app do and how are its components organized
        - how is data distributed, what does the app do?
    - how can data be broken up based on CQRS (command query responsibility segregation) pattern? What belongs on control plane and what belongs on data plane?
    - how does the app scale + what components drive needed capacity?
    - does app have schedule-based tasks
    - are workers listening to a queue? (easy kill w SQSQ)
    - where can refactoring or enhanced functionality occur w/o impacting current implmentation 
- three factors when comparing cost of ownership
    - infrastructure cost to run workload (e.g. EC2 capacity vs per-invocation cost of Lambda functions)
    - development effort to plan, architect, and provision resources that app runs on 
    - cost of team's time to maintain app when in prod 
## Choosing Compute Services and Data Stores 
- use Fargate and Lambda for serverless compute 
    - considerations
        - Fargate: more of a lift-and-shift style move to serverless 
            - better for consistent traffic (e.g. longer-running processes, consistent workload)
            - need more than 3GB memory(?)
            - app w non-HTTP(S) listener
            - need to run side cars w service
            - container image portability w docker runtime
        - Lambda
            - better for tasks that run in <15 minutes
            - spiky, unpredictable workloads
            - unknown demand
            - lighter-weight, app-focused stateless computing
            - simplified IT automation
            - real-time data processing
            - reduced complexity for development and ops 
        - combination of the two might make the most sense 
    - data stores
        - use different data stores for different data needs
        - transactional vs query operations 
        - DDB: transactional
        - S3 (data lakes, economical state store, claim-check pattern, filter data retrieved by lambda w S3 select)
        - Elasticache for Redis (great for real-time leaderboards)
        - Amazon Quantum Ledger DB (QLDB)
            - model state changes in cryptographically provable manner
            - distributed ledger 
        - RDBs
            - aurora: transactional
            - aurora serverless
                - best for inconsistent use
            - RDS
        - challenges of multiple data stores
            - have to manage partial executions (could use SAGA pattern for step functions)
            - source of truth for one piece of data must be shared w other domains that use that data 
                - choose a timely ETL method
                    - Glue + S3 + Athena + Redshift spectrum
                    - bash scripts 
## Application Architecture Patterns
- serverless IT automation
    - replace cron job w timed lambda functions 
    - replace routine tasks w step functions 
    - use Lambda to prevent unwanted configuration changes 
- serverless web apps and mobile apps 

## Exam review
- dead-letter queues
    - redrive policy defines source queue, dead-letter queue, and when to move messages to DLQ
        - redrive allow policy: specifies which source Qs can access DLQ
    - maxReceiveCount: # of times a consumer tries receiving message from Q w/o deleting it (i.e., no work gets done) before being moved to DLQ
- autoscaling
    - default: terminates instance first in AZ with greater number of instances, then instance w oldest launch template, then instance closest to next billing hour s
- DDB r/w capacity modes
    - on-demand
        - instantly accommodate workloads as they ramp up or down
        deliver single-digit ms latency at any scale 
        - throttling can occur if double the previous peak is exceeded w/n 30 minutes 
    - provisioned (default, free-tier eligible)
        - specify number of reads/write/s app requires 
- DDB: think metadata (among other things)
- EBS: can modify from io2 to gp3 using EC2 ModifyVolume action w/o downtime 
- RDS: to encrypt a snapshot of an unencrypted DB, take a snapshot, copy the snapshot as an encrypted snapshot, then restore DB from that snapshot 
    - can't create encrypted read replica of an unencrypted DB
    - can't create encrypted backup of unencrypted DB 
    - RDS (and EBS) snapshots stored in S3 cannot be viewed in the customer console 
- elasticache: write-through adds or updates data in cache whenever app writes data to db
    - lazy loading: only loads data into cache when necessary
- global accelerator: improves TCP and UDP network performance for users around the world, even if an app is hosted in one region
- TGW: can be the connection point of site-to-site VPN and can use equal cost multipath routing to increase throughput of the VPN copnnection 
- SNS: topics aren't stored & cannot be re-examined if processing fails 
- **get clarification: does the exam still test for the info that only egress-only IGWs support IPv6 and NAT gateways don't?**
- VPC: every instance launched in dual-stack vpc has IPv4 and IPv6 addresses, so it's possible to run out, since the IPv4 are fairly finite 
- athena: doesn't like files smaller than 128MB
    - partition data by date and region in S3 to help reduce amount of data athena has to scan for each query 
    - use Glue ETL to convert .csv to Parquet for columnar data storage, which can help increase athena query performance 
- WAF: common web attacks (cross-site scripting + SQL injection)
- Shield: DDoS 
- redshift: OLAP (analytics processing)  
- neptune: graph DB 
